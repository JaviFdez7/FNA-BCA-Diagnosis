---
title: "R Notebook"
output:
  html_notebook: default
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

---
title: "FID-G5"
output: html_notebook
---

# An치lisis de Datos de Im치genes FNA para el Diagn칩stico de C치ncer de Mama

**Autores:** Claudia Heredia Ceballos, Manuel Otero Barbas치n, Marta Pineda Gisbert, Javier Fern치ndez Castillo.

**Grupo:** G5.

**Organizaci칩n**: Universidad de sevilla.

## Resumen

Este proyecto tiene como objetivo la aplicaci칩n de t칠cnicas de aprendizaje supervisado y no supervisado para el diagn칩stico de c치ncer de mama, utilizando un conjunto de datos que contiene caracter칤sticas extra칤das de im치genes de aspirado con aguja fina (FNA) de masas mamarias. El conjunto de datos contiene 569 muestras, con 357 benignas y 212 malignas, y 33 caracter칤sticas que describen diversas propiedades de los n칰cleos celulares en las im치genes. En este estudio, se explorar치n varios enfoques de clasificaci칩n supervisada, como **CART**, **Random Forest** y **SVM**, para predecir el diagn칩stico de las muestras, adem치s de aplicar t칠cnicas no supervisadas como el **clustering** y la **reducci칩n de dimensionalidad** a trav칠s de **PCA**. Se espera que el an치lisis combinado de ambos enfoques permita mejorar la precisi칩n en el diagn칩stico y proporcionar una mejor comprensi칩n de los patrones subyacentes en los datos. **Se discutir치n los resultados obtenidos y se presentar치n conclusiones sobre la efectividad de las t칠cnicas empleadas.**

## 1. Introducci칩n

El diagn칩stico temprano y preciso del c치ncer de mama es crucial para un tratamiento exitoso. En este estudio, se emplea el conjunto de datos [**Breast Cancer Wisconsin (Diagnostic) Data Set**](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29), que contiene caracter칤sticas derivadas de im치genes FNA de c칠lulas de tejido mamario.

El FNA es una t칠cnica de diagn칩stico utilizada para evaluar la naturaleza de las masas mamarias. Durante el procedimiento, se extrae una peque침a cantidad de tejido de la masa y se examina bajo un microscopio para determinar si es benigna o maligna. Las caracter칤sticas de las c칠lulas, como la forma, el tama침o y la textura de los n칰cleos, pueden proporcionar informaci칩n valiosa sobre la naturaleza del tumor.

En la **Figura 1** se ilustra el procedimiento de aspiraci칩n con aguja fina (FNA), que muestra la inserci칩n de la aguja para extraer las c칠lulas del tumor. La **Figura 2** presenta una imagen microsc칩pica de las c칠lulas extra칤das, permitiendo observar sus caracter칤sticas una vez realizado el FNA.

::: {style="text-align: center;"}
<img src="images/fine-needle-aspiration-using-ultrasound.jpg" alt="Procedimiento de Aspiraci칩n con Aguja Fina (FNA)" style="width: 35%;"/> <br> <b>Figura 1:</b><i>Procedimiento FNA</i>
:::

::: {style="text-align: center;"}
<img src="images/cells.jpg" alt="C칠lulas extra칤das de FNA" style="width: 35%;"/> <br> <b>Figura 2:</b><i> C칠lulas extra칤das de FNA</i>
:::

El conjunto de datos incluye 33 caracter칤sticas, como el radio, la textura, el per칤metro y la simetr칤a de los n칰cleos celulares, las cuales se utilizan para determinar si una muestra es **benigna** o **maligna**. El an치lisis se centra en la aplicaci칩n de t칠cnicas de aprendizaje supervisado, como **CART**, **Random Forest** y **SVM**, para predecir el diagn칩stico basado en las caracter칤sticas num칠ricas. Adem치s, se aplicar치n t칠cnicas de aprendizaje no supervisado, como el **clustering** y la **reducci칩n de dimensionalidad** mediante **PCA** , para explorar los patrones ocultos en los datos y posiblemente mejorar la precisi칩n de los modelos.

## 2. Metodolog칤a

El desarrollo de este proyecto se llevar치 a cabo a trav칠s de un enfoque estructurado y secuencial en varias fases, con el objetivo de analizar y predecir el diagn칩stico de c치ncer de mama utilizando im치genes FNA. Los pasos clave del proceso son los siguientes:

1.  **An치lisis Inicial y Preprocesamiento de los Datos**\
    En esta fase inicial, se proceder치 con la carga de los datos del conjunto de datos "Breast Cancer Wisconsin (Diagnostic)", que contiene las caracter칤sticas extra칤das de im치genes de aspiraci칩n con aguja fina (FNA). Se realizar치 una exploraci칩n preliminar de los datos para comprender la naturaleza de las variables y la distribuci칩n de las clases (benigno y maligno). Adem치s, se identificar치 y gestionar치 cualquier valor faltante, y se evaluar치 la necesidad de realizar transformaciones adicionales, como la normalizaci칩n o la conversi칩n de variables categ칩ricas.

2.  **Aplicaci칩n de Modelos Supervisados**\
    En esta etapa, se implementar치n y entrenar치n varios **modelos supervisados** para la clasificaci칩n de los tumores como benignos o malignos. Los modelos seleccionados incluyen:

    -   **CART (Classification and Regression Trees)**: Un modelo de 치rbol de decisiones que permite realizar predicciones mediante una serie de reglas basadas en los datos de entrada.
    -   **Random Forest**: Un conjunto de 치rboles de decisi칩n que mejora la precisi칩n mediante el uso de t칠cnicas de muestreo y combinaci칩n de varios modelos.
    -   **SVM (Support Vector Machine)**: Un modelo que encuentra el hiperplano 칩ptimo que separa las clases de manera eficaz.
    -   **GLM (Generalized Linear Model)**: Un modelo estad칤stico que puede adaptarse a diferentes distribuciones de las variables dependientes, como la binomial en este caso.\
        Se evaluar치 la **Importancia de las Variables** en los modelos para identificar las caracter칤sticas m치s influyentes en las predicciones.

3.  **Experimentaci칩n con el Dataset Reducido**\
    Posteriormente, se realizar치 una **experimentaci칩n** en la que se modificar치n las caracter칤sticas del dataset mediante la eliminaci칩n de variables menos relevantes, bas치ndose en la **importancia de las variables** obtenida de los modelos. Esto permitir치 optimizar los modelos y reducir el sobreajuste, mejorando la capacidad de generalizaci칩n del sistema de diagn칩stico.

------------------------------------------------------------------------

4.  **Aplicaci칩n de Modelos No Supervisados**\
    Para explorar los patrones subyacentes en los datos sin utilizar etiquetas de clase, se implementar치n **modelos no supervisados**. Estos incluir치n:
    -   **Clustering**: Se utilizar치n t칠cnicas como **K-means** para agrupar las observaciones de acuerdo con similitudes en sus caracter칤sticas. Esto permitir치 identificar posibles grupos de datos que podr칤an no haber sido evidentes en la clasificaci칩n supervisada.
    -   **Reducci칩n de Dimensionalidad**: Para reducir el n칰mero de variables y simplificar el an치lisis, se emplear치n t칠cnicas como **PCA (Principal Component Analysis)**. Estas t칠cnicas ayudar치n a descubrir las estructuras latentes en los datos y facilitar치n la visualizaci칩n y el an치lisis posterior.
5.  **Presentaci칩n de los Resultados y Conclusiones**\
    Finalmente, se presentar치n los **resultados** obtenidos de los modelos evaluados, incluyendo tablas, gr치ficos y an치lisis comparativos. En esta secci칩n se discutir치 la efectividad de las t칠cnicas empleadas, destacando los modelos m치s precisos y los enfoques que mejor se ajustan a las necesidades del diagn칩stico de c치ncer de mama. Las **conclusiones** se centrar치n en la viabilidad y los posibles pasos a seguir para mejorar el rendimiento del modelo y su aplicabilidad en un entorno cl칤nico.

## 3. Implementaci칩n

En esta secci칩n se muestran los pasos que se han seguido para la realizaci칩n del proyecto. Incluye el An치lisis Inicial y Preprocesamiento de los Datos, los modelos supervisados y no supervisados utilizados.

### 3.1. An치lisis Inicial y Preprocesamiento de los Datos.

#### 3.1.1. Importaci칩n de librer칤as

Se instalan los paquetes que ser치n necesarios durante el proyecto:

-   Tidyverse: Para la manipulaci칩n de datos y gr치ficos.
-   Caret: Para el preprocesamiento y modelado Lattice es requerido por Caret
-   DataExplorer: Para la exploraci칩n automatizada de los datos.
-   Dplyr: Proporciona una gram치tica de manipulaci칩n de datos.
-   Ggplot2: Personalizaci칩n de gr치ficas.
-   Psych: Para an치lisis estad칤stico
-   Corrplot: Visualizaci칩n de matriz de correlaci칩n.
-   Car:  Para complementar t칠cnicas de regresi칩n.
-   pRoc: Proporciona herramientas para visualizar., suavizar y comparar curvas Roc.

```{r setup, warning=FALSE, message=FALSE}
# Nota: Descomentar las l칤neas de instalaci칩n si no se tienen los paquetes instalados. Comando ctrl+shift+c para descomentar.

#install.packages("tidyverse")
#install.packages("caret")
#install.packages("DataExplorer")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("lattice")
#install.packages("psych")
#install.packages("corrplot")
#install.packages("ggcorrplot")
#install.packages("car")
#install.packages("pROC")
library(tidyverse)
library(caret)
library(DataExplorer)
library(dplyr)
library(ggplot2)
library(lattice)
library(psych)
library(corrplot)
library(ggcorrplot)
library(pROC)
library(car)
```

#### 3.1.2. Carga y visualizaci칩n de datos

Para comenzar el an치lisis es necesario realizar la carga de los datos, se visualizan las primeras filas y la estructura de nuestro dataset.

```{r}
data <- read.csv("data/data.csv")

head(data)
```



```{r}
dim <- dim(data)

cat("N칰mero de columnas:", dim[2], "\n")
cat("N칰mero de filas:", dim[1], "\n")
```
Como podemos observar, nuestro dataset cuenta con 569 filas y 33 columnas.


A continuaci칩n, se muestra la estructura, tipos y algunos ejemplos de las variables del dataset para tener una visi칩n global del las distintas variables con las que se cuenta.

```{r}
str(data)
```

```{r}
# sapply aplica una funci칩n a cada columna del dataframe
clases <- sapply(data, class)
clases_df <- data.frame(Clase = clases)

print(clases_df, row.names = FALSE)
```
Se obtienen las clases de cada columna en el conjunto de datos para entender mejor su tipo de datos y estructura. Esto permite visualizar r치pidamente el tipo de variable (num칠rica, categ칩rica, etc.) que se encuentra en el dataset.


Se visualiza un resumen est치distico de los datos.

```{r}
describe(data)
```




Se verifica si existen valores faltantes en los datos utilizando la funci칩n `anyNA()`. Esto permite identificar r치pidamente si hay datos incompletos en el dataset que puedan requerir limpieza o manejo especial.

```{r}
anyNA(data)
```

Se observa que existen valores faltantes en el dataset. Para visualizar de manera m치s clara la cantidad de valores faltantes por columna, se utiliza la funci칩n `plot_missing(data)`.

```{r}
plot_missing(data)
```

Como puede observarse, se cuenta con 569 valores faltantes en la 칰ltima columna "X". M치s adelante se proceder치 a eliminar esta columna.

#### 3.1.3. An치lisis exploratorio de los datos

En este paso el objetivo ser치 entender la distribuci칩n y relaciones de variables.

##### 3.1.3.1. Visualizaci칩n de distribuciones y correlaciones:

Se visualiza un resumen gr치fico general para entender las distribuciones y correlaciones entre las variables en el conjunto de datos. Esto ayuda a identificar patrones, distribuciones de frecuencia y posibles relaciones entre las diferentes columnas del dataset.

```{r}
plot_intro(data)
```

Como se puede observar, el dataset cuenta con un 3% de columnas discretas, en concreto la columna "diagnosis" que es la variable objetivo, y un 94% de columnas continuas. Adem치s, se observa un 3% de columnas con valores faltantes, que corresponden a la columna "X".

Se visualizan las variables categ칩ricas que dice si el tumor es "Maligno" o "Benigno" para contar cu치ntas observaciones hay de cada tipo. Esto permite ver la distribuci칩n de la columna diagnosis y entender la proporci칩n entre ellas en el dataset.

```{r}
diagnosis_counts <- table(data$diagnosis)
plot_bar(data$diagnosis)
print("Distribuci칩n de la variable 'diagnosis' Benigno (B) y Maligno (M):")
print(diagnosis_counts)
```

La columna "diagnosis" contiene dos clases: "B" (Benigno) y "M" (Maligno). La distribuci칩n de las clases es desigual, con 357 observaciones de la clase "B" y 212 observaciones de la clase "M". Esto indica un desequilibrio ligero en la distribuci칩n de las clases.

Para visualizar la distribuci칩n de las variables num칠ricas, se utiliza la funci칩n `plot_histogram(data)`. Esto permite ver la distribuci칩n de cada variable en el dataset y comprender mejor la variabilidad y rango de valores de las caracter칤sticas.

```{r}
plot_histogram(data)
```

Como se puede observar, las variables num칠ricas presentan diferentes distribuciones y rangos de valores. Algunas variables, como "concave.points_worst", parecen tener una distribuci칩n sesgada hacia la derecha, mientras que otras, como "symmetry_mean", parecen tener una distribuci칩n m치s sim칠trica. Estas diferencias en las distribuciones pueden ser 칰tiles para identificar patrones y relaciones entre las variables.

#### 3.1.4. Preprocesamiento de los datos

Es necesario realizar un preprocesamiento de los datos para limpiarlos y prepararlos para el an치lisis y modelado. Esto incluye la eliminaci칩n de valores faltantes, la codificaci칩n de variables categ칩ricas y la selecci칩n de atributos relevantes.

##### 3.1.4.1. Eliminaci칩n de valores faltantes

Para comenzar, se ha identificado que hay una columna con todos los valores faltantes, es decir, NA. El primer paso en el preprocesamiento ser치 eliminar esta columna "x".

Se verifica que la columna "X" se haya eliminado correctamente y que no haya otros valores faltantes en el dataset.

```{r}
data <- data %>% select(-X)
plot_missing(data)
anyNA(data)
```

No quedan valores faltantes en el dataset, por lo que se proceder치 a eliminar la columna "id", ya que no aporta informaci칩n relevante.

```{r}
data <- data %>% select(-id)

print("Busqueda de columna id en el dataset:")
print("id" %in% colnames(data))

```

Como se puede observar, se ha eliminado la columna "id" y se ha verificado que no hay m치s valores faltantes, excepto los ya eliminados en "X".

Despu칠s de estos procesos, contamos con un dataset de:

```{r}
dim <- dim(data)

cat("N칰mero de columnas:", dim[2], "\n")
cat("N칰mero de filas:", dim[1], "\n")
```

569 filas y 31 columnas.

##### 3.1.4.2. Codificaci칩n de variables Categ칩ricas

Anteriormente, vimos que nuestro dataset cuenta con una 칰nica columna de valores categ칩ricos, "Diagnosis", cuyos valores tienen el siguiente significado: - M (malignant) - B (benign)

El siguiente paso en el preprocesado de datos ser치 pasar esta columna a num칠rica, lo que ser치 necesario para estudiar la correlaci칩n de variables de nuestro dataset. 

Como solo se presentan dos posibles valores ("M" y "B"), se aplicar치 Codificaci칩n Binaria: El valor "M" pasar치 a ser 1 y valor "B" pasar치 a ser 0.


```{r}

#M -> 1; B -> 0
data$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)
```

Se verifica que la columna "diagnosis" se haya codificado correctamente.

```{r}
# imprimir los primeros valores de la columna diagnosis, formateado
print("Valores de la columna 'diagnosis' despu칠s de la codificaci칩n:")
data$diagnosis
```

A continuaci칩n, se verifica que todas las columnas del dataset sean num칠ricas, ya que algunos modelos de aprendizaje autom치tico requieren que todas las variables de entrada lo sean. Para ello, se identifican las columnas categ칩ricas y se comprueba que todas las columnas sean num칠ricas.

```{r}

categorical_columns <- sapply(data, is.factor) | sapply(data, is.character)
names(data)[categorical_columns]


```

Efectivamente, todas las columnas ahora son num칠ricas, lo que da paso al siguiente punto en el preprocesamiento de datos.

##### 3.1.4.3. Estudio de correlaci칩n.

Al tener el dataset limpio de valores faltantes, NA, y solo hay presentes variables num칠ricas, el siguiente paso ser치 estudiar las posibles correlaciones de nuestro dataset.

Estudiar la correlaci칩n de los datos ayuda a identificar patrones y relaciones entre variables, lo que podr칤a conducir a nuevas hip칩tesis y descubrimientos. Adem치s, un buen estudio de correlaciones podr칤a ser 칰til para seleccionar variables relevantes y construir modelos en un futuro.

Los valores que se obtienen tendr치n la siguiente interpretaci칩n:

-   Correlaci칩n cercana a 1: Relaci칩n positiva fuerte.
-   Correlaci칩n cercana a -1: Relaci칩n negativa fuerte.
-   Correlaci칩n cercana a 0: No hay relaci칩n lineal significativa

Se calcula la matriz de correlaci칩n para el conjunto de datos utilizando solo las observaciones completas. Esto permite visualizar las relaciones lineales entre las variables num칠ricas del dataset, identificando patrones de correlaci칩n positiva y negativa entre las diferentes variables.

```{r}

# Calcular matriz de correlaci칩n
correlation_matrix <- cor(data, use = "complete.obs")  # Ignora valores faltantes


# Graficar matriz de correlaci칩n
corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.8)

```

Como se puede observar, dado que el conjunto de datos contiene 31 variables, la matriz de correlaci칩n resulta dif칤cil de interpretar directamente. Para facilitar la comprensi칩n de las relaciones entre las variables, se muestra a continuaci칩n una tabla con las parejas de atributos ordenadas por la magnitud de la correlaci칩n.

```{r}
# Eliminar la columna 'diagnosis' antes de calcular las correlaciones
data_no_target <- data[, !names(data) %in% c("diagnosis")]

cor_matrix <- cor(data_no_target, use = "complete.obs")

cor_matrix_df <- as.data.frame(as.table(cor_matrix))
cor_matrix_df <- cor_matrix_df[cor_matrix_df$Var1 != cor_matrix_df$Var2, ]
cor_matrix_df <- cor_matrix_df[order(-abs(cor_matrix_df$Freq)), ]
# Eliminar duplicados. a<->b = b<->a
cor_matrix_df <- cor_matrix_df[seq(1, nrow(cor_matrix_df), by = 2), ]

print(cor_matrix_df)
```

En la tabla se presentan las correlaciones entre las variables independientes del conjunto de datos. Se observan correlaciones altas entre algunas de las variables, como **perimeter_mean y radius_mean** (0.998), **perimeter_worst y radius_worst** (0.994), **area_mean y radius_mean** (0.987), y **area_worst y perimeter_worst** (0.978). Estas altas correlaciones tienen sentido porque las variables de **radio** (radius) y **per칤metro** (perimeter) est치n matem치ticamente relacionadas: a medida que el radio de un tumor aumenta, tambi칠n lo hace su per칤metro, lo que explica las correlaciones cercanas a **1**. Similarmente, las variables de **치rea** tambi칠n est치n fuertemente correlacionadas con el radio y el per칤metro, ya que un tumor con un mayor radio suele tener una mayor 치rea. Aunque estas correlaciones son altas, no son sorprendentes, ya que todas est치n capturando diferentes aspectos del **tama침o** y la **forma** del tumor.

Dado que "diagnosis" es nuestra variable objetivo, vamos a observar c칩mo se correlacionan las dem치s variables con ella para entender mejor su relaci칩n y posibles influencias.

```{r}
cor_with_target <- cor(data, data$diagnosis, use = "complete.obs")
correlation_df <- data.frame(Variable = names(data), Correlation = cor_with_target)
correlation_df_sorted <- correlation_df[order(-correlation_df$Correlation), ]
print(correlation_df_sorted)
```

En esta tabla se presenta la correlaci칩n de cada variable con la variable objetivo **'diagnosis'**. La correlaci칩n de **'diagnosis'** consigo misma es **1**, como es esperado. Las variables que presentan las correlaciones m치s altas con **'diagnosis'** son **concave.points_worst** (0.793), **perimeter_worst** (0.782), **concave.points_mean** (0.776), **radius_worst** (0.776) y **perimeter_mean** (0.743). Estas correlaciones indican que las caracter칤sticas relacionadas con el **n칰mero de puntos c칩ncavos** y el **per칤metro** del tumor est치n fuertemente asociadas con el diagn칩stico, lo que las convierte en buenos predictores del diagn칩stico maligno o benigno. En general, las variables que presentan correlaciones superiores a **0.7** sugieren que son relevantes para la predicci칩n de la variable objetivo. Estas caracter칤sticas del **tama침o** y la **forma** del tumor, como el per칤metro y los puntos c칩ncavos, son especialmente importantes para predecir si el tumor es maligno o benigno.

Las variables menos correlacionadas con **'diagnosis'** son **symmetry_se** (-0.006), **fractal_dimension_se** (-0.077), **texture_se** (-0.008), **smoothness_se** (-0.067) y **fractal_dimension_mean** (-0.012). Estas correlaciones cercanas a **0** indican que estas caracter칤sticas no est치n fuertemente asociadas con el diagn칩stico y pueden no ser tan relevantes para la predicci칩n. Esto tiene sentido ya que son errores est치ndar y medidas de textura y suavidad que podr칤an no ser tan informativas para distinguir entre tumores malignos y benignos.

##### 3.1.4.4. Selecci칩n de Atributos

La selecci칩n de atributos se refiere al proceso de identificar y elegir las variables m치s relevantes para un an치lisis, descartando las redundantes o irrelevantes para mejorar la calidad y eficiencia del modelo. En este caso, dado que algunas variables en el dataset muestran altas correlaciones entre s칤, se podr칤a considerar eliminarlas para reducir la dimensionalidad y simplificar el an치lisis.
88
A pesar de las correlaciones fuertes entre ciertas variables, ninguna es mayor de 0,8-0.9 por lo que eliminarlas podr칤a no ser adecuado ya que el conjunto de datos es limitado. La eliminaci칩n de variables podr칤a llevar a una p칠rdida significativa de informaci칩n valiosa que podr칤a ser crucial para el an치lisis y las predicciones. Mantener todas las variables como "area_mean" o "perimeter_mean" aporta informaci칩n 칰nica y valiosa al modelo, lo que permite capturar un panorama m치s completo del tumor.

Por lo tanto, se optar치 por mantener todas las variables.

### 3.2. An치lisis Supervisado.

El aprendizaje supervisado, tambi칠n conocido como machine learning supervisado, es una subcategor칤a del machine learning y la inteligencia artificial. Se define por el uso de conjuntos de datos etiquetados para entrenar algoritmos que clasifican los datos o predicen los resultados con precisi칩n.

A medida que se introducen datos en el modelo, este ajusta sus ponderaciones iterativamente hasta que se ha alcanzado un ajuste adecuado, proceso que ocurre como parte de la validaci칩n cruzada.

En este an치lisis, la variable a predecir ser치 "diagnosis", que como se mencion칩 previamente, es binaria: 1 si el tumor es maligno y 0 si es benigno.

Se evaluaron diferentes modelos de clasificaci칩n para un conjunto de datos con 569 muestras y 30 variables, categorizados en 'Benigno' y 'Maligno'. Los modelos analizados incluyen el 츼rbol de Decisi칩n (CART), Random Forest (Bosque Aleatorio), M치quinas de Soporte Vectorial con n칰cleo Radial (SVM), Modelo Lineal Generalizado (GLM) y un clasificador con redes neuronales.

El primer paso en el an치lisis supervisado es realizar la divisi칩n del dataset.

#### 3.2.1. Divisi칩n del Dataset:

Para este paso, se utilizar치 la validaci칩n cruzada K-fold para dividir el dataset y evaluar el modelo. En lugar de una divisi칩n simple en entrenamiento y prueba, K-fold asegura que cada subconjunto o fold del dataset sea utilizado tanto para el entrenamiento como para la prueba en diferentes iteraciones, proporcionando as칤 una evaluaci칩n m치s robusta.

Establecemos una semilla para asegurar que los resultados sean reproducibles, es decir, que se obtengan los mismos resultados cada vez que se ejecute el c칩digo. Luego, configuramos el K-fold Cross-Validation con probabilidades de clase, que nos permitir치 evaluar los modelos utilizando m칠tricas de clasificaci칩n binaria.

```{r}
set.seed(123)

k <- 5

train_control <- trainControl(
  method = "cv",           # Cross-validation
  number = k,              # N칰mero de pliegues (folds)
  classProbs = TRUE,       # Habilitar probabilidades de clase
  summaryFunction = twoClassSummary, # Para m칠tricas de clasificaci칩n binaria
  savePredictions = "final" # Guardar las predicciones finales
)

```

Se cambian los niveles de la variable objetivo "diagnosis" de 0 y 1 a "B" y "M" para facilitar la interpretaci칩n de los resultados y la visualizaci칩n de las m칠tricas de evaluaci칩n.

```{r}
data$diagnosis <- factor(data$diagnosis, levels = c(0, 1), labels = c("B", "M"))
```

Para poder evaluar el rendimiento de los modelos, es necesario establecer un punto de referencia o *baseline* que nos permita comparar su desempe침o. En este caso, utilizaremos un enfoque simple basado en la clase mayoritaria, que consiste en predecir la clase m치s com칰n en el conjunto de datos sin tener en cuenta ninguna caracter칤stica predictiva. Este enfoque no es 칰til para la predicci칩n real, pero nos proporcionar치 una referencia m칤nima para evaluar la eficacia de los modelos posteriores.

En nuestro an치lisis, la clase mayoritaria se identifica como "Benigno" (B). Esto significa que, si utiliz치ramos este enfoque, todas las instancias se clasificar칤an como benignas, ignorando cualquier caracter칤stica del conjunto de datos que pudiera diferenciar los casos malignos (M). Este enfoque nos proporciona una medida b치sica de rendimiento, incluyendo m칠tricas como la *exactitud (accuracy)* y la sensibilidad para la clase mayoritaria.

Si un modelo m치s avanzado no puede superar este baseline en m칠tricas clave, como el 치rea bajo la curva ROC (AUC-ROC) o la precisi칩n general, entonces el modelo no estar칤a capturando patrones significativos en los datos. Por lo tanto, no ser칤a 칰til para la clasificaci칩n de los tumores de mama.

El ROC muestra la tasa de verdaderos positivos frente a la tasa de falsos positivos a diferentes umbrales de clasificaci칩n, mientras que el AUC proporciona un 칰nico valor que resume el rendimiento del modelo en t칠rminos de probabilidad de clasificaci칩n. Estos valores se utilizar치n posteriormente para comparar con otros modelos, ayudando a evaluar cu치l proporciona las mejores predicciones en t칠rminos de sensibilidad y especificidad.

Para el baseline se identifica a continuaci칩n la clase mayoritaria en el dataset. Luego se predice esta clase en todos los casos y se calcula la matriz de confusi칩n para evaluar el desempe침o de la predicci칩n basada en la clase mayoritaria.

```{r}
# Identificar la clase mayoritaria
majority_class <- as.character(names(sort(table(data$diagnosis), decreasing = TRUE)[1]))

# Predicci칩n de la clase mayoritaria en todos los casos
majority_predictions <- factor(rep(majority_class, nrow(data)), levels = levels(data$diagnosis))

conf_matrix_majority <- confusionMatrix(majority_predictions, data$diagnosis)
cat("Baseline: Predicci칩n de la clase mayoritaria\n")
print(conf_matrix_majority)
```

Se calcula el valor del ROC (Receiver Operating Characteristic) y el AUC (츼rea Bajo la Curva) para evaluar el rendimiento de la predicci칩n basada en la clase mayoritaria.

```{r warning=FALSE}


# Calcular la probabilidad de cada clase basada en las frecuencias relativas
class_probabilities <- prop.table(table(data$diagnosis))

# Crear probabilidades para el baseline
baseline_probabilities <- rep(class_probabilities[majority_class], nrow(data))

# Calcular el ROC y el AUC
roc_curve <- roc(response = data$diagnosis, 
                 predictor = baseline_probabilities, 
                 levels = rev(levels(data$diagnosis)))

cat("Valor del AUC-ROC para el baseline:", auc(roc_curve), "\n")

```

Los resultados obtenidos para el baseline basado en la clase mayoritaria revelan las limitaciones de este enfoque simplista. El modelo clasifica correctamente todos los casos benignos (B), logrando una sensibilidad perfecta de 1.0 para esta clase. Sin embargo, su especificidad es 0.0, lo que indica que no logra identificar ning칰n caso maligno (M).

La *exactitud* general del modelo es del 62.74%, lo que coincide con la proporci칩n de la clase mayoritaria en el conjunto de datos. Este valor representa el *No Information Rate (NIR)*, o la precisi칩n esperada si las predicciones se hicieran de manera aleatoria bas치ndose 칰nicamente en la distribuci칩n de clases. Adem치s, el valor de kappa es 0, reflejando que las predicciones no aportan informaci칩n m치s all치 del azar.

El resultado m치s significativo es la incapacidad del modelo para identificar correctamente los casos malignos. Esto se evidencia por un *Valor Predictivo Positivo (PPV)* de 62.74%, pero un *Valor Predictivo Negativo (NPV)* indefinido (NaN), ya que nunca predice la clase M. Adem치s, el 치rea bajo la curva ROC (AUC-ROC) es de 0.5, lo que indica que el modelo no es capaz de discriminar entre las dos clases.

Este baseline denota la necesidad de utilizar modelos m치s avanzados que sean capaces de capturar patrones discriminativos en los datos para lograr un mejor equilibrio entre sensibilidad y especificidad. Este punto de partida servir치 como referencia m칤nima para evaluar la eficacia de los modelos posteriores.

#### 3.2.2. 츼rbol de Decisi칩n (CART):

El 치rbol de decisi칩n es un modelo simple que divide los datos en segmentos basados en reglas de decisi칩n. Es 칰til para clasificaciones donde las decisiones son l칩gicas y f치ciles de entender

```{r}

# 츼rbol de Decisi칩n utilizando K-fold cross-validation
model_cart <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "rpart",     # 츼rbol de decisi칩n (CART)
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_cart)

# Extraer la importancia de las variables
importance_cart <- varImp(model_cart, scale = FALSE)
importance_cart_df <- importance_cart$importance
importance_cart_df$variable <- rownames(importance_cart_df)

```

El primer modelo analizado es el 츼rbol de Decisi칩n (CART). Este modelo utiliza un valor de complejidad de poda (cp) de 0.0047, que fue seleccionado como el mejor par치metro mediante validaci칩n cruzada.

La curva ROC del modelo es de 0.9362, lo que indica una alta capacidad para discriminar entre las dos clases.

La sensibilidad (capacidad del modelo para identificar correctamente las observaciones positivas) es de 0.9383, lo que sugiere que el modelo es muy eficiente para detectar las observaciones positivas, aunque algo menos efectivo que otros modelos en cuanto a la especificidad. De hecho, la especificidad (capacidad para identificar correctamente las observaciones negativas) es de 0.8960, lo que representa una leve ca칤da respecto a la sensibilidad.

Este desempe침o es s칩lido y equilibrado, pero no es el m치s alto entre los modelos evaluados.

#### 3.2.3. Random Forest

El Random Forest es un algoritmo que construye m칰ltiples 치rboles de decisi칩n y realiza una predicci칩n agregando las predicciones de todos los 치rboles individuales. Es robusto ante el sobreajuste.

```{r}
model_rf <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "rf",        # Random Forest
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_rf)

# Extraer la importancia de las variables
importance_rf <- varImp(model_rf, scale = FALSE)
importance_rf_df <- importance_rf$importance
importance_rf_df$variable <- rownames(importance_rf_df)

```

Para entender mejor los resultados del modelo, aclarar que el par치metro mtry en el contexto de Random Forest es uno de los hiperpar치metros clave que se utiliza para controlar el n칰mero de variables (caracter칤sticas) que el modelo considera para dividir cada nodo en cada 치rbol del bosque. Espec칤ficamente, mtry define cu치ntas caracter칤sticas ser치n elegidas aleatoriamente para cada nodo cuando se construye un 치rbol en el Random Forest.

Random Forest, muestra un desempe침o destacable. Este modelo seleccion칩 el valor de mtry (n칰mero de variables aleatorias para cada divisi칩n del 치rbol) igual a 2, lo que optimiza la capacidad de discriminaci칩n. Su curva ROC alcanza un valor impresionante de 0.9908, lo que es un indicador claro de su capacidad para separar las dos clases con gran precisi칩n. Adem치s, la sensibilidad de 0.9804 muestra que Random Forest tiene una excelente capacidad para detectar correctamente las observaciones positivas, y la especificidad de 0.9241 indica que tambi칠n es eficaz en identificar las observaciones negativas. Este modelo sobresale por su alta precisi칩n en ambos aspectos, lo que lo convierte en uno de los modelos m치s robustos y confiables para este conjunto de datos.

#### 3.2.4. Support Vector Machine (SVM)

El Support Vector Machine (SVM) es un algoritmo que intenta encontrar el hiperplano que mejor separe las diferentes clases de datos.

```{r}
#install.packages("kernlab")
library(kernlab)

model_svm <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "svmRadial",  # Support Vector Machine con kernel radial
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_svm)

# Extraer la importancia de las variables
importance_svm <- varImp(model_svm, scale = FALSE)
importance_svm_df <- importance_svm$importance
importance_svm_df$variable <- rownames(importance_svm_df)

```

El tercer modelo evaluado es el de M치quinas de Soporte Vectorial con n칰cleo Radial (SVM). Este modelo, con un par치metro de regularizaci칩n 洧냤=1 y un valor de sigma de 0.0475, mostr칩 un desempe침o excelente en t칠rminos de la curva ROC, alcanzando un valor de 0.9948, el m치s alto entre todos los modelos. Esta m칠trica refleja una capacidad de discriminaci칩n superior, lo que implica que el modelo tiene una alta habilidad para separar correctamente las clases 'Negative' y 'Positive'. La sensibilidad de 0.9748 y la especificidad de 0.9670 tambi칠n son notablemente altas, lo que sugiere que el modelo tiene un buen rendimiento tanto en la detecci칩n de las observaciones positivas como en la correcta identificaci칩n de las negativas. Sin embargo, es importante destacar que el modelo SVM present칩 varias advertencias durante el proceso de optimizaci칩n (warnings), relacionadas con problemas de convergencia y probabilidades extremas de 0 o 1. Esto podr칤a indicar que el modelo podr칤a estar sobreajustando o enfrentando dificultades para encontrar un equilibrio estable, lo que debe tenerse en cuenta al evaluar su estabilidad y generalizaci칩n.

#### 3.2.5. Regresi칩n Log칤stica

Este algoritmo es un modelo de clasificaci칩n que predice la probabilidad de que una observaci칩n pertenzca a una clase o no.

```{r warning=FALSE}


model_logit <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "glm",       # Regresi칩n Log칤stica
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_logit)

# Extraer la importancia de las variables
importance_logit <- varImp(model_logit, scale = FALSE)
importance_logit_df <- importance_logit$importance
importance_logit_df$variable <- rownames(importance_logit_df)

```

El 칰ltimo modelo considerado es el Modelo Lineal Generalizado (GLM). Este modelo, a pesar de ser sencillo en su estructura, mostr칩 un rendimiento respetable. Su curva ROC alcanz칩 un valor de 0.9552, lo cual es inferior a los de Random Forest y SVM, pero sigue siendo adecuado para tareas de clasificaci칩n. La sensibilidad de 0.9438 indica que el modelo tiene una buena capacidad para identificar las observaciones positivas, mientras que la especificidad de 0.9484 es ligeramente mejor que la sensibilidad, lo que sugiere que el modelo tiene un desempe침o ligeramente mejor para detectar las observaciones negativas en comparaci칩n con las positivas. Aunque el modelo GLM es funcional, su rendimiento en t칠rminos de la curva ROC es algo inferior en comparaci칩n con los modelos m치s complejos como SVM y Random Forest.

#### 3.2.6. Red Neuronal

Las redes neuronales son un modelo de aprendizaje profundo que imita el funcionamiento del cerebro humano. Est치n compuestas por capas de neuronas interconectadas que procesan la informaci칩n y aprenden a partir de los datos. Puede ser interesante evaluar el rendimiento de una red neuronal en comparaci칩n con los modelos tradicionales de aprendizaje supervisado.

```{r}
# Entrenar una red neuronal para clasificaci칩n binaria
model_nn <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "nnet",      # Red Neuronal
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC",       # Evaluar utilizando AUC (츼rea bajo la curva ROC)
  verbose = FALSE       # Suprimir los detalles del entrenamiento
)


# Imprimir los resultados del modelo
print(model_nn$results)


# Extraer la importancia de las variables
importance_nn <- varImp(model_nn, scale = FALSE)
importance_nn_df <- importance_nn$importance
importance_nn_df$variable <- rownames(importance_nn_df)


```
#### 3.2.7 Evaluaci칩n de los modelos

Esta secci칩n se dedica a evaluar el rendimiento de los modelos utilizados en el an치lisis. La comparaci칩n de modelos se realiza utilizando t칠cnicas de validaci칩n cruzada, como K-fold, para obtener una visi칩n robusta del desempe침o de cada uno. Los resultados obtenidos de estos modelos se comparan a trav칠s del resumen de resampling y se visualizan mediante el diagrama bwplot para identificar cu치l modelo ofrece mejor precisi칩n y generalizaci칩n en las predicciones.

```{r}
resamples <- resamples(list(cart = model_cart, rf = model_rf, svm = model_svm, logit = model_logit, nn = model_nn))

summary(resamples)
bwplot(resamples)

```

Se observa que el mejor modelo es el "SVM" ya que tiene el valor m치s alto de AUC-ROC, adem치s de tener una alta sensibilidad y especificidad. El modelo "Random Forest" tambi칠n tiene un buen rendimiento, pero la especificidad no es tan alta probablemente por el outlier que se observa en el diagrama bwplot.

Redes neuronales tambien cuenta con buen desempe침o aunque con menor sensibilidad y especificidad que los modelos anteriores. Logit y CART presentan un rendimiento inferior en comparaci칩n con los otros modelos.

### 3.3. Experimentaci칩n de modelos supervisados con reducci칩n de variables

En esta secci칩n, se experimentar치 con la reducci칩n de variables para evaluar si es posible mejorar el rendimiento de los modelos de aprendizaje supervisado. La reducci칩n de variables implica seleccionar un subconjunto de caracter칤sticas m치s relevantes y eliminar las menos importantes, lo que puede simplificar el modelo y mejorar su capacidad predictiva. 


Se proceder치 a realizar un estudio de la importancia de las variables para identificar cu치les son las caracter칤sticas m치s relevantes en la predicci칩n del diagn칩stico de c치ncer de mama y as칤 poder seleccionar las m치s importantes para mejorar el rendimiento de los modelos.

#### 3.3.1. Estudio de la Importancia de las Variables

El estudio de la importancia de variables nos permite identificar cu치les son las caracter칤sticas que m치s influyen en las predicciones realizadas.

El objetivo del estudio de importancia de variables es eliminar aquellas que consistentemente tienen baja importancia en todos los modelos:

1- Obtener la importancia de variables de cada modelo. 2- Normalizar la importancia para compararla entre modelos. 3- Calcular una m칠trica consolidada de importancia promedio. 4- Identificar las variables con menor impacto en todos los modelos.

##### 3.3.1.1. 츼rbol de Decisi칩n (CART)

En los 치rboles de decisi칩n, la importancia de vairables se calcula en funci칩n de las ganancias de reducci칩n de impureza, por ejemplo, la reducci칩n de la entrop칤a o del 칤ndice Gini en los nodos donde la variables es utilizada para dividr los datos.

```{r}
plot(importance_cart, main = "Importancia de Variables - CART")
```

Las variables con mayor importancia en el modelo CART son "concave.points_worst", "concave.points_mean", "radius_worst", "area_worst" y "perimeter_worst". Estas variables son las m치s influyentes en la predicci칩n del diagn칩stico de c치ncer de mama, lo que sugiere que las caracter칤sticas relacionadas con los puntos c칩ncavos, el radio, el 치rea y el per칤metro del tumor son cr칤ticas para distinguir entre tumores malignos y benignos.

##### 3.3.1.2. Random Forest

En Random Forest la importancia se calcula mediante dos enfoques comunes:

-   Importancia basada en permutaci칩n: Eval칰a c칩mo cambia la precisi칩n del modelo al permutar aleatoriamente los valores de una variable.
-   Reducci칩n promedio de la impureza: Calcula cu치nto contribuye una variable a la reducci칩n de impureza a trav칠s de todos los 치rboles del bosque.

```{r}
plot(importance_rf, main = "Importancia de Variables - Random Forest")
```

En este caso las variables m치s importantes son similares al modelo anterior.

##### 3.3.1.3. Support Vector Machine (SVM)

El calculo de la importancia en SVM no es tan directo, ya que este modelo no se basa en una estructura jer치rquica o en una gregaci칩n de 치rboles. Podemos estimar la importancia de las variables mediante an치lisis post-hoc, como la evaluaci칩n de los coeficientes en el espacio formado por el n칰cleo radial.

```{r}
plot(importance_svm, main = "Importancia de Variables- SVM")
```

Al igual que en los modelos anteriores, parece que las variables de peor y media de perimetro radio area y concavidad son las m치s importantes en la predicci칩n del diagn칩stico de c치ncer de mama.

##### 3.3.1.4. Regresi칩n Log칤stica

En regresi칩n log칤stica, la importancia de variables se puede analizar mediante los coeficientes estimados del modelo. Estos coeficientes indican la magnitud y la direcci칩n del efecto de cada variable en la probabilidad de que un tumor sea maligno.

```{r}
plot(importance_logit, main = "Importancia de Variables - GLM")
```

En este caso la simetr칤a toma un papel importante en la predicci칩n del diagn칩stico de c치ncer de mama.

##### 3.3.1.5 Red Neuronal

En las redes neuronales, la importancia de las variables puede ser m치s dif칤cil de interpretar debido a la complejidad del modelo. Sin embargo, es posible analizar la contribuci칩n de cada variable a la salida de la red mediante t칠cnicas de backpropagation y an치lisis de sensibilidad.

```{r}
plot(importance_nn, main = "Importancia de Variables - Red Neuronal")
```

Al igual que en los modelos anteriores, las variables relacionadas con el tama침o y la forma del tumor, como el radio, el 치rea y el per칤metro, parecen ser las m치s influyentes en la predicci칩n del diagn칩stico de c치ncer de mama.

Para **normalizar la importancia** y poder compararla entre modelos, se debe calcular la **importancia relativa** de cada variable en t칠rminos de contribuci칩n a la predicci칩n del modelo. Esto permite **estandarizar las mediciones** y hacerlas comparables, independientemente del modelo o del algoritmo usado. Luego, se calcula una **m칠trica consolidada** que represente la **importancia promedio** de las variables en todos los modelos. Este valor consolidado se obtiene al promediar las **importancias individuales** de cada variable entre los diferentes modelos. Por 칰ltimo, se identifican las **variables con menor impacto** en todos los modelos al analizar aquellas que tienen una importancia **significativamente baja** en comparaci칩n con otras.

```{r}
# Consolidar importancia de variables
importance_combined <- merge(
  merge(importance_cart_df, importance_rf_df, by = "variable", suffixes = c("_cart", "_rf")),
  merge(importance_svm_df, importance_logit_df, by = "variable", suffixes = c("_svm", "_logit")),
  
  by = "variable"
)

importance_combined <- merge(importance_combined, importance_nn_df, by = "variable")

# Promedio de importancia
importance_combined$mean_importance <- rowMeans(importance_combined[, -1], na.rm = TRUE)

# Seleccionar las menos importantes (por debajo de un umbral, por ejemplo, el percentil 20)
threshold <- quantile(importance_combined$mean_importance, 0.2)
least_important_vars <- importance_combined$variable[importance_combined$mean_importance <= threshold]

# Eliminar estas variables del dataset original
data_reduced <- data[, !(names(data) %in% least_important_vars)]

# Guardar el dataset reducido
write.csv(data_reduced, "data_reduced.csv", row.names = FALSE)

```

A continuaci칩n, se presentan las variables menos importantes en los modelos evaluados y el dataset reducido.

```{r}
print("Variables menos importantes:")
print(least_important_vars)
print("")
print("Dataset reducido:")
print(dim(data_reduced))

```

Como se puede observar, las variables menos importantes en los modelos evaluados son "symmetry_se", "fractal_dimension_se", "texture_se", "smoothness_se" y "fractal_dimension_mean". Estas variables tienen una baja contribuci칩n a la predicci칩n del diagn칩stico en comparaci칩n con otras caracter칤sticas m치s relevantes, como el tama침o y la forma del tumor. El dataset reducido contiene 25 variables.

#### 3.3.2. Entrenamiento de Modelos con Dataset Reducido

El siguiente paso ser치 repetir el an치lisis supervisado esta vez usando el dataset reducido en el que no aparecen las columnas "menos importantes". De esta forma podremos analizar si los resultados mejorar, empeoran o no afectan con el estudio de importancia de varianles:

**츼rbol de Decisi칩n (CART)**

```{r}
# 츼rbol de Decisi칩n utilizando K-fold cross-validation
model_cart_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "rpart",     # 츼rbol de decisi칩n (CART)
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_cart_reduced)
```

**Random Forest**

```{r}
model_rf_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "rf",        # Random Forest
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_rf_reduced)

```

**Support Vector Machine (SVM)**

```{r}
#install.packages("kernlab")
library(kernlab)

model_svm_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "svmRadial",  # Support Vector Machine con kernel radial
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_svm_reduced)

```

**Regresi칩n Log칤stica**

```{r}
model_logit_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "glm",       # Regresi칩n Log칤stica
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_logit_reduced)
```

**Red Neuronal**

```{r}
# Entrenar una red neuronal para clasificaci칩n binaria
model_nn_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "nnet",      # Red Neuronal
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC",       # Evaluar utilizando AUC (츼rea bajo la curva ROC)
  verbose = FALSE       # Suprimir los detalles del entrenamiento
)

```

#### 3.3.3. Comparaci칩n de resultados an치lisis supervisado dataset incial vs dataset reducido:

A continuaci칩n, se comparar치n los resultados de los modelos entrenados con el dataset completo y el dataset reducido. Se evaluar치n las m칠tricas de rendimiento, como el 치rea bajo la curva ROC (AUC-ROC), la sensibilidad y la especificidad, para determinar si la reducci칩n de variables afecta el rendimiento de los modelos.

```{r}
# Comparativa entre los modelos
# Lista de los modelos completos y reducidos
modelos_completos <- list(cart = model_cart, rf = model_rf, svm = model_svm, logit = model_logit, nn = model_nn)
modelos_reducidos <- list(cart = model_cart_reduced, rf = model_rf_reduced, svm = model_svm_reduced, logit = model_logit_reduced, nn = model_nn_reduced)

# Inicializamos un data frame vac칤o para almacenar los resultados
resultados <- data.frame(
  modelo = character(),
  roc_inicial = numeric(),
  sens_inicial = numeric(),
  spec_inicial = numeric(),
  roc_reducido = numeric(),
  sens_reducido = numeric(),
  spec_reducido = numeric(),
  stringsAsFactors = FALSE
)

# Iteramos sobre los modelos completos y reducidos
for (nombre in names(modelos_completos)) {
  # Modelo completo
  modelo_completo <- modelos_completos[[nombre]]
  mejor_completo <- modelo_completo$results[which.max(modelo_completo$results$ROC), c("ROC", "Sens", "Spec")]
  
  # Modelo reducido
  modelo_reducido <- modelos_reducidos[[nombre]]
  mejor_reducido <- modelo_reducido$results[which.max(modelo_reducido$results$ROC), c("ROC", "Sens", "Spec")]
  
  # Agregar al data frame
  resultados <- rbind(resultados, data.frame(
    modelo = nombre,
    roc_inicial = mejor_completo$ROC,
    sens_inicial = mejor_completo$Sens,
    spec_inicial = mejor_completo$Spec,
    roc_reducido = mejor_reducido$ROC,
    sens_reducido = mejor_reducido$Sens,
    spec_reducido = mejor_reducido$Spec,
    stringsAsFactors = FALSE
  ))
}

# Imprimir los resultados
print(resultados)

# Una tabla mostrando el relative change de cada m칠trica
comparativa <- resultados %>%
  mutate(
    roc_change = (roc_reducido - roc_inicial) / roc_inicial ,
    sens_change = (sens_reducido - sens_inicial) / sens_inicial,
    spec_change = (spec_reducido - spec_inicial) / spec_inicial
  ) %>%
  select(modelo, roc_change, sens_change, spec_change)

flextable(comparativa)

```

Como se puede ver en la tabla, que muestra el cambio relativo en las m칠tricas de rendimiento de los modelos completos y reducidos, la reducci칩n de variables no afecta significativamente el rendimiento de los modelos. En general, los cambios en el 치rea bajo la curva ROC (AUC-ROC), la sensibilidad y la especificidad son m칤nimos, lo que sugiere que las variables eliminadas no ten칤an un impacto significativo en la capacidad predictiva de los modelos. Esto indica que el dataset reducido sigue siendo capaz de capturar los patrones relevantes en los datos y de realizar predicciones precisas sobre el diagn칩stico de c치ncer de mama.

Como los modelos no mejoran ni empeoran significativamente con la reducci칩n de variables, se puede concluir que el dataset original contiene informaci칩n redundante o poco relevante para la predicci칩n del diagn칩stico. Adem치s, ahora es un proceso m치s eficiente y menos costoso computacionalmente, ya que se trabaja con un n칰mero menor de variables.

### 3.4. An치lisis No Supervisado.

El an치lisis no supervisado tiene como objetivo identificar patrones ocultos o estructuras presentes en los datos sin requerir etiquetas. Para este prop칩sito, se emplean t칠cnicas de clustering y reducci칩n de dimensionalidad que permiten explorar la informaci칩n contenida en las caracter칤sticas de las c칠lulas. Estas t칠cnicas facilitan la identificaci칩n de grupos de observaciones similares y proporcionan una perspectiva m치s clara sobre las relaciones entre las variables.

#### 3.4.1. Preparaci칩n del Dataset

Previo a la aplicaci칩n de t칠cnicas de an치lisis no supervisado, es fundamental realizar un preprocesamiento adecuado de los datos. El primer paso consiste en eliminar la columna "diagnosis", ya que dicha variable contiene la etiqueta que se desea predecir y no se utiliza en este tipo de an치lisis. Posteriormente, se normalizan las caracter칤sticas, dado que muchas t칠cnicas, como el clustering, son sensibles a las escalas de las variables. Este proceso asegura que todas las variables tengan la misma influencia en el modelo.

```{r}
data_UnSupervised <- data %>% select(-diagnosis)
head(data_UnSupervised)
data_UnSupervised$diagnosis
```

La normalizaci칩n asegura que las variables con diferentes unidades de medida no dominen el an치lisis y evita que el algoritmo de clustering o reducci칩n de dimensionalidad se vea sesgado por la magnitud de las variables. Se utiliza la funci칩n preProcess para calcular los par치metros de normalizaci칩n y con la funci칩n predict, se aplican los par치metros calculados previamente al conjunto de datos, generando una nueva versi칩n normalizada de las caracter칤sticas

```{r}
# Normalizar los datos 
preprocess_params <- preProcess(data_UnSupervised[, -ncol(data_UnSupervised)], method = c("center", "scale"))
data_normalized <- predict(preprocess_params, data_UnSupervised[, -ncol(data_UnSupervised)])

```

El primer m칠todo de an치lisis no supervisado que se emplea es el clustering, el cual tiene como prop칩sito agrupar las observaciones en funci칩n de sus similitudes.

#### 3.4.2. Clustering: Determinaci칩n del N칰mero 칍ptimo de Clusters

En el an치lisis de clustering, es fundamental determinar el n칰mero 칩ptimo de grupos o clusters, ya que este par치metro impacta directamente en la calidad de la segmentaci칩n. Existen diversas metodolog칤as para identificar este valor, las cuales se basan en criterios estad칤sticos y geom칠tricos que eval칰an la estructura de los datos y la cohesi칩n de los clusters formados.

El primer paso para aplicar t칠cnicas de clustering consiste en identificar el n칰mero adecuado de clusters. Dos m칠todos comunes para este prop칩sito son:

M칠todo del codo: Este enfoque eval칰a la variaci칩n explicada en funci칩n del n칰mero de clusters. Se identifica el "codo" en la gr치fica, que corresponde al punto donde la mejora en la varianza explicada se estabiliza, indicando que a침adir m치s clusters no produce beneficios significativos.

칈ndice de Silhouette: Este 칤ndice mide la similitud de cada punto con su propio cluster en comparaci칩n con otros clusters. Valores cercanos a +1 sugieren que los puntos est치n correctamente agrupados, mientras que valores cercanos a -1 indican posibles errores en la asignaci칩n de clusters.

```{r}
# M칠todo del codo
wss <- (nrow(data_normalized) - 1) * sum(apply(data_normalized, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(data_normalized, centers = i)$withinss)

plot(1:15, wss, type = "b", xlab = "N칰mero de Clusters", ylab = "Suma de Cuadrados Internos")

# 칈ndice silhouette
library(cluster)
silhouette_scores <- numeric()
for (i in 2:15) {
  km <- kmeans(data_normalized, centers = i)
  silhouette_scores[i] <- mean(silhouette(km$cluster, dist(data_normalized))[, 3])
}
plot(2:15, silhouette_scores[-1], type = "b", xlab = "N칰mero de Clusters", ylab = "Puntaje de Silhouette")

```

El siguiente m칠todo de an치lisis no supervisado que se utiliza es el algoritmo K-means, el cual permite agrupar los datos en un n칰mero espec칤fico de clusters previamente determinado.

#### 3.4.3. K-Means

El algoritmo K-means es una t칠cnica de clustering que organiza los datos en K clusters, con el objetivo de minimizar la varianza dentro de cada grupo. Aunque se trata de un m칠todo ampliamente utilizado, la interpretaci칩n de los resultados debe realizarse con cautela, ya que el n칰mero de clusters es sensible tanto a la inicializaci칩n como a la distribuci칩n de los datos.

```{r}
set.seed(123)

optimal_clusters = 4

kmeans_model <- kmeans(data_normalized, centers = optimal_clusters, nstart = 25)

# Agregar etiquetas de cluster al dataset original
data$cluster <- kmeans_model$cluster

# Mostrar los clusters de diferentes formas:

# Tabla de frecuencias
table(data$cluster)

# Resumen de los datos agrupados por cluster
aggregate(data[, -ncol(data)], by = list(cluster = data$cluster), FUN = mean)


```

El siguiente paso en nuestro an치lisis no supervisado es aplicar el An치lisis de Componentes Principales (PCA). Esta t칠cnica permitir치 visualizar la estructura de los datos, as칤 como validar los resultados obtenidos a trav칠s del clustering.

#### 3.4.4. PCA

La dimensionalidad se reducir치 transformando las variables originales en componentes principales para identificar la mayor parte de la variabilidad en los datos. Esto facilita la visualizaci칩n de la estructura subyacente y optimiza la interpretaci칩n de los resultados del clustering. PCA simplifica el an치lisis al reducir la complejidad de los datos.

```{r}
# PCA
pca_model <- prcomp(data_normalized, scale = TRUE)

# Visualizaci칩n de los dos primeros componentes principales
pca_data <- data.frame(pca_model$x[, 1:2], cluster = as.factor(data$cluster))
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Clustering visualizado en espacio PCA")
table(data$diagnosis, data$cluster)


```

Se visualizan claramente cuatro cl칰steres.

Los clusters est치n bien definidos y separados espacialmente, especialmente entre el cluster p칰rpura y los dem치s.

Existe un grado menor de solapamiento entre los clusters, lo que sugiere que las variables originales ofrecen una separaci칩n m치s clara entre las clases en este espacio.

En cuanto al espacio de variaci칩n, los puntos est치n distribuidos en coordenadas centradas, con valores que van de -15 a 5 en PC1 y de -10 a 10 en PC2.

Tambi칠n se repetir치 el clustering con el dataset reducido generado durante el estudio de la importancia de las variables.

#### 3.4.5. An치lisis No Supervisado con Dataset Reducido seg칰n importancia de variables

```{r}
data_UnSupervised_reduced <- data_reduced %>% select(-diagnosis)
head(data_UnSupervised_reduced)
data_UnSupervised_reduced$diagnosis
```

```{r}
# Normalizar los datos (sin la variable de respuesta si existe)
preprocess_params <- preProcess(data_UnSupervised_reduced[, -ncol(data_UnSupervised_reduced)], method = c("center", "scale"))
data_normalized_reduced <- predict(preprocess_params, data_UnSupervised_reduced[, -ncol(data_UnSupervised_reduced)])
```

```{r}
# M칠todo del codo
wss <- (nrow(data_normalized_reduced) - 1) * sum(apply(data_normalized_reduced, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(data_normalized_reduced, centers = i)$withinss)

plot(1:15, wss, type = "b", xlab = "N칰mero de Clusters", ylab = "Suma de Cuadrados Internos")

# 칈ndice silhouette
library(cluster)
silhouette_scores <- numeric()
for (i in 2:15) {
  km <- kmeans(data_normalized_reduced, centers = i)
  silhouette_scores[i] <- mean(silhouette(km$cluster, dist(data_normalized_reduced))[, 3])
}
plot(2:15, silhouette_scores[-1], type = "b", xlab = "N칰mero de Clusters", ylab = "Puntaje de Silhouette")
```

k-means

```{r}
set.seed(123)

optimal_clusters_2 = 5

kmeans_model_reduced <- kmeans(data_normalized_reduced, centers = optimal_clusters_2, nstart = 25)

# Agregar etiquetas de cluster al dataset original
data_reduced$cluster <- kmeans_model_reduced$cluster
```

```{r}
# PCA
pca_model_reduced <- prcomp(data_normalized_reduced, scale = TRUE)

# Visualizaci칩n de los dos primeros componentes principales
library(ggplot2)
pca_data_reduced <- data.frame(pca_model_reduced$x[, 1:2], cluster = as.factor(data_reduced$cluster))
ggplot(pca_data_reduced, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Clustering visualizado en espacio PCA")
```

En este caso, se visualizan cinco clusters, lo que podr칤a indicar una mayor sensibilidad del modelo al agrupar los datos reducidos. Los clusters est치n m치s compactos en el centro del gr치fico, con menos separaci칩n clara en comparaci칩n con la versi칩n completa. Adem치s, hay cl칰steres adicionales (como el cluster 5, representado en rosa), lo que podr칤a indicar una partici칩n m치s forzada o una sensibilidad al ruido.

Existe un mayor solapamiento entre los clusters en comparaci칩n con el gr치fico sin reducci칩n de datos. El rango del eje PC1 var칤a de -5 a 15, mientras que PC2 tiene una distribuci칩n de -5 a 15, lo que sugiere un cambio en la importancia relativa de los componentes principales despu칠s de la reducci칩n de variables.

En conclusi칩n, sin reducci칩n, los clusters son m치s definidos y separados, lo que sugiere que las variables eliminadas contienen informaci칩n relevante para distinguir los grupos. Con reducci칩n, los clusters muestran mayor solapamiento y parecen menos definidos, posiblemente debido a la p칠rdida de informaci칩n al reducir las variables. La aparici칩n de un quinto cluster en el conjunto reducido indica que la reducci칩n puede introducir sensibilidad al ruido o revelar patrones m치s complejos (aunque menos significativos).

El siguiente algoritmo utilizado en el an치lisis no supervisado es el Apriori para reglas de asociaci칩n.

#### 3.4.6. Reglas de asociaci칩n con algoritmo Apriori.

Las reglas de asociaci칩n se emplean en Data Mining para identificar relaciones entre elementos de un conjunto de datos. Estas relaciones ayudan a encontrar patrones en los datos y est치n formadas por un antecedente {a} y un consecuente {b}, indicando que {a} -\> {b}. Es relevante conocer t칠rminos como:

Soporte: Proporci칩n de transacciones que contienen ambos conjuntos de 칤tems (antecedente y consecuente). Confianza: Indicador de cu치n bien predice el antecedente al consecuente.

*Algoritmo Apriori*

Se utilizar치 el Algoritmo Apriori para obtener las reglas de asociaci칩n, ya que resulta 칰til para identificar itemsets frecuentes en datos transaccionales, como eventos registrados en un intervalo de tiempo determinado.

##### Fase 1: Reducci칩n del n칰mero de candidatos

1.  Primero, se generan todos los itemsets con un 칰nico 칤tem. Luego, estos itemsets se combinan para formar itemsets con dos elementos, y as칤 sucesivamente. Se seleccionar치n 칰nicamente los pares cuyo soporte sea mayor o igual a un umbral minsup, eliminando aquellos que no cumplan con este criterio.

Se instalar치 el paquete necesario (arules), que es un paquete en R que proporciona funcionalidades para trabajar con reglas de asociaci칩n.

```{r}
#install.packages("arules")
library(arules)
```

2.  Se cargan los datos como transacciones en el formato `basket`, interpretando cada transacci칩n como un conjunto de 칤tems (basket). Esto permite manejar tanto datos categ칩ricos (como diagnosis) como continuos (los otros atributos) en la misma transacci칩n.

```{r}
transacciones <- read.transactions("data_reduced.csv", format = "basket", sep = ",")

```

##### Fase 2: Generar reglas Par치metros: soporte m칤nimo (0.01 -\> 1%) y confianza m칤nima (80%)

En esta fase, se proceder치 a generar las reglas de asociaci칩n utilizando los par치metros de soporte m칤nimo (0.01 - 1%) y confianza m칤nima (80%). Estos valores permiten filtrar las reglas que son suficientemente frecuentes y confiables, asegurando que se obtengan relaciones significativas entre los 칤tems en los datos.

```{r}
reglasAsociacion <- apriori(transacciones, parameter = list(supp = 0.01, conf = 0.8))

```

Se filtran las reglas basadas en el lift y la confianza, de modo que solo se consideren aquellas con un lift mayor a 1.5 y una confianza superior al 80%. Un lift \> 1.5 indica una fuerte asociaci칩n entre los 칤tems en la regla, mientras que una confianza 곤 0.8 asegura que la probabilidad de que el consecuente ocurra cuando el antecedente est치 presente es alta.

```{r}
filtradas <- subset(reglasAsociacion, lift > 1.5 & confidence >= 0.8)
inspect(filtradas)
```

### 4. Conclusiones

Hemos llevado a cabo un an치lisis exhaustivo de un conjunto de datos de c치ncer de mama, que incluye la exploraci칩n de datos, la construcci칩n de modelos de aprendizaje supervisado y no supervisado, as칤 como la identificaci칩n de reglas de asociaci칩n. Algunos hallazgos clave son los siguientes:

-   Agrupaci칩n en los datos de c치ncer de mama: Se detectaron patrones de agrupaci칩n mediante t칠cnicas de clustering y reducci칩n de dimensionalidad. Dos de los cuatro cl칰steres identificados en el an치lisis no supervisado son 100% malignos, lo cual podr칤a ser de especial inter칠s para estudios futuros.

-   Relevancia de las reglas de asociaci칩n: Se identificaron reglas de asociaci칩n significativas entre las variables del conjunto de datos, lo que ofrece informaci칩n valiosa sobre las relaciones entre las caracter칤sticas celulares y el diagn칩stico de c치ncer de mama.

-   Aprendizaje supervisado: Evaluamos varios modelos, incluyendo 츼rboles de Decisi칩n, Random Forest, SVM, Regresi칩n Log칤stica y Redes Neuronales. Los modelos Random Forest y SVM destacaron con un rendimiento excepcional, logrando altas puntuaciones de AUC y sensibilidad. La reducci칩n de variables menos importantes no afect칩 significativamente el desempe침o de los modelos, lo que sugiere que las variables eliminadas no aportaban informaci칩n relevante y probablemente no introduc칤an ruido, dado el alto grado de correlaci칩n entre las variables.

En general, este an치lisis proporciona informaci칩n valiosa para la detecci칩n y el diagn칩stico del c치ncer de mama, resaltando la importancia de utilizar una variedad de t칠cnicas de an치lisis de datos para lograr una comprensi칩n completa de los patrones y relaciones en los datos.

### 5. Referencias

-   [UCI Machine Learning Repository: Breast Cancer Wisconsin (Diagnostic) Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))
-   Asignatura de Fundamentos de ingenier칤a de datos de la Universidad de Sevilla.
