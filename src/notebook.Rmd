---
title: "R Notebook"
output:
  html_notebook: default
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---

---
title: "FID-G5"
output: html_notebook
---

# An치lisis de Datos de Im치genes FNA para el Diagn칩stico de C치ncer de Mama

**Autores:** Claudia Heredia Ceballos, Manuel Otero Barbas치n, Marta Pineda Gisbert, Javier Fern치ndez Castillo.

**Organizaci칩n**: Universidad de sevilla.

## Resumen

Este proyecto tiene como objetivo la aplicaci칩n de t칠cnicas de aprendizaje supervisado y no supervisado para el diagn칩stico de c치ncer de mama, utilizando un conjunto de datos que contiene caracter칤sticas extra칤das de im치genes de aspirado con aguja fina (FNA) de masas mamarias. El conjunto de datos contiene 569 muestras, con 357 benignas y 212 malignas, y 30 caracter칤sticas num칠ricas que describen diversas propiedades de los n칰cleos celulares en las im치genes. En este estudio, se explorar치n varios enfoques de clasificaci칩n supervisada, como **CART**, **Random Forest** y **SVM**, para predecir el diagn칩stico de las muestras, adem치s de aplicar t칠cnicas no supervisadas como el **clustering** y la **reducci칩n de dimensionalidad** a trav칠s de **PCA**. Se espera que el an치lisis combinado de ambos enfoques permita mejorar la precisi칩n en el diagn칩stico y proporcionar una mejor comprensi칩n de los patrones subyacentes en los datos. **Se discutir치n los resultados obtenidos y se presentar치n conclusiones sobre la efectividad de las t칠cnicas empleadas.**

## 1. Introducci칩n

El diagn칩stico temprano y preciso del c치ncer de mama es crucial para un tratamiento exitoso. En este estudio, se emplea el conjunto de datos [**Breast Cancer Wisconsin (Diagnostic) Data Set**](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29), que contiene caracter칤sticas derivadas de im치genes FNA de c칠lulas de tejido mamario.

El FNA es una t칠cnica de diagn칩stico utilizada para evaluar la naturaleza de las masas mamarias. Durante el procedimiento, se extrae una peque침a cantidad de tejido de la masa y se examina bajo un microscopio para determinar si es benigna o maligna. Las caracter칤sticas de las c칠lulas, como la forma, el tama침o y la textura de los n칰cleos, pueden proporcionar informaci칩n valiosa sobre la naturaleza del tumor.


En la **Figura 1** se ilustra el procedimiento de aspiraci칩n con aguja fina (FNA), que muestra la inserci칩n de la aguja para extraer las c칠lulas del tumor. La **Figura 2** presenta una imagen microsc칩pica de las c칠lulas extra칤das, permitiendo observar sus caracter칤sticas una vez realizado el FNA.

<div style="text-align: center;">
  <img src="images/fine-needle-aspiration-using-ultrasound.jpg" alt="Procedimiento de Aspiraci칩n con Aguja Fina (FNA)" style="width: 35%;"/>
  <br>
  <b>Figura 1:</b><i>Procedimiento FNA</i>
</div>

<div style="text-align: center;">
  <img src="images/cells.jpg" alt="C칠lulas extra칤das de FNA" style="width: 35%;"/>
  <br>
  <b>Figura 2:</b><i> C칠lulas extra칤das de FNA</i>
</div>


El conjunto de datos incluye 30 caracter칤sticas, como el radio, la textura, el per칤metro y la simetr칤a de los n칰cleos celulares, las cuales se utilizan para determinar si una muestra es **benigna** o **maligna**. El an치lisis se centra en la aplicaci칩n de t칠cnicas de aprendizaje supervisado, como **CART**, **Random Forest** y **SVM**, para predecir el diagn칩stico basado en las caracter칤sticas num칠ricas. Adem치s, se aplicar치n t칠cnicas de aprendizaje no supervisado, como el **clustering** y la **reducci칩n de dimensionalidad** mediante **PCA** , para explorar los patrones ocultos en los datos y posiblemente mejorar la precisi칩n de los modelos.

## 2. Metodolog칤a

El desarrollo de este proyecto se llevar치 a cabo a trav칠s de un enfoque estructurado y secuencial en varias fases, con el objetivo de analizar y predecir el diagn칩stico de c치ncer de mama utilizando im치genes FNA. Los pasos clave del proceso son los siguientes:

1. **An치lisis Inicial y Preprocesamiento de los Datos**  
   En esta fase inicial, se proceder치 con la carga de los datos del conjunto de datos "Breast Cancer Wisconsin (Diagnostic)", que contiene las caracter칤sticas extra칤das de im치genes de aspiraci칩n con aguja fina (FNA). Se realizar치 una exploraci칩n preliminar de los datos para comprender la naturaleza de las variables y la distribuci칩n de las clases (benigno y maligno). Adem치s, se identificar치 y gestionar치 cualquier valor faltante, y se evaluar치 la necesidad de realizar transformaciones adicionales, como la normalizaci칩n o la conversi칩n de variables categ칩ricas.


2. **Aplicaci칩n de Modelos Supervisados**  
   En esta etapa, se implementar치n y entrenar치n varios **modelos supervisados** para la clasificaci칩n de los tumores como benignos o malignos. Los modelos seleccionados incluyen:  
   - **CART (Classification and Regression Trees)**: Un modelo de 치rbol de decisiones que permite realizar predicciones mediante una serie de reglas basadas en los datos de entrada.
   - **Random Forest**: Un conjunto de 치rboles de decisi칩n que mejora la precisi칩n mediante el uso de t칠cnicas de muestreo y combinaci칩n de varios modelos.  
   - **SVM (Support Vector Machine)**: Un modelo que encuentra el hiperplano 칩ptimo que separa las clases de manera eficaz.  
   - **GLM (Generalized Linear Model)**: Un modelo estad칤stico que puede adaptarse a diferentes distribuciones de las variables dependientes, como la binomial en este caso.  
   Se evaluar치 la **Importancia de las Variables** en los modelos para identificar las caracter칤sticas m치s influyentes en las predicciones.

3. **Experimentaci칩n con el Dataset Reducido**  
   Posteriormente, se realizar치 una **experimentaci칩n** en la que se modificar치n las caracter칤sticas del dataset mediante la eliminaci칩n de variables menos relevantes, bas치ndose en la **importancia de las variables** obtenida de los modelos. Esto permitir치 optimizar los modelos y reducir el sobreajuste, mejorando la capacidad de generalizaci칩n del sistema de diagn칩stico.


---

4. **Aplicaci칩n de Modelos No Supervisados**  
   Para explorar los patrones subyacentes en los datos sin utilizar etiquetas de clase, se implementar치n **modelos no supervisados**. Estos incluir치n:  
   - **Clustering**: Se utilizar치n t칠cnicas como **K-means**  para agrupar las observaciones de acuerdo con similitudes en sus caracter칤sticas. Esto permitir치 identificar posibles grupos de datos que podr칤an no haber sido evidentes en la clasificaci칩n supervisada.  
   - **Reducci칩n de Dimensionalidad**: Para reducir el n칰mero de variables y simplificar el an치lisis, se emplear치n t칠cnicas como **PCA (Principal Component Analysis)**. Estas t칠cnicas ayudar치n a descubrir las estructuras latentes en los datos y facilitar치n la visualizaci칩n y el an치lisis posterior.


5. **Presentaci칩n de los Resultados y Conclusiones**  
   Finalmente, se presentar치n los **resultados** obtenidos de los modelos evaluados, incluyendo tablas, gr치ficos y an치lisis comparativos. En esta secci칩n se discutir치 la efectividad de las t칠cnicas empleadas, destacando los modelos m치s precisos y los enfoques que mejor se ajustan a las necesidades del diagn칩stico de c치ncer de mama. Las **conclusiones** se centrar치n en la viabilidad y los posibles pasos a seguir para mejorar el rendimiento del modelo y su aplicabilidad en un entorno cl칤nico.



## 3. Desarrollo

En esta secci칩n se muestran los pasos que se han seguido para la realizaci칩n del proyecto. Incluye el An치lisis Inicial y Preprocesamiento de los Datos, los modelos supervisados y no supervisados utilizados.




### 3.1. An치lisis Inicial y Preprocesamiento de los Datos. 

#### Importaci칩n de librer칤as

Instalamos los paquetes que ser치n necesarios durante nuestro proyecto:

-   Tidyverse: Para la manipulaci칩n de datos y gr치ficos.
-   Caret: Para el preprocesamiento y modelado Lattice es requerido por Caret
-   DataExplorer: Para la exploraci칩n automatizada de los datos.
-   Dplyr: Proporciona una gram치tica de manipulaci칩n de datos.
-   Ggplot2: Personalizaci칩n de gr치ficas.
-   Psych: Para an치lisis estad칤stico
-   Corrplot: Visualizaci칩n de matriz de correlaci칩n.


```{r}
# Nota: Descomentar las l칤neas de instalaci칩n si no se tienen los paquetes instalados. Comando ctrl+shift+c para descomentar.

#install.packages("tidyverse")
#install.packages("caret")
#install.packages("DataExplorer")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("lattice")
#install.packages("psych")
#install.packages("corrplot")
#install.packages("ggcorrplot")

library(caret)
library(DataExplorer)
library(dplyr)
library(tidyverse)
library(psych)
library(corrplot)
library(ggcorrplot)




```
#### Carga de datos

Para comenzar nuestro an치lisis es necesario realizar la carga de los datos, visualizamos las primeras filas y la estructura de nuestro dataset:

```{r}
data <- read.csv("data/data.csv")

#primeras filas de nuestro dataset:
head(data)

```


```{r}
# Dimensi칩n de nuestro dataset:
dim(data)
```

Nuestro dataset cuenta con 33 columnas y 569 filas.

```{r}
# Con str mostramos la estructura de nuestro dataframe, incluyendo los tipos de nuestras variables:
str(data)
```

```{r}

# Con Describe podemos ver un primer resumen estad칤stico b치sico:

describe(data)

```

```{r}
# Tipos de datos en nuestras variables:
sapply(data, class)
```

```{r}
# Veamos si existen valores faltantes en nuestros datos:

anyNA(data)
```

```{r}
#Contamos el numero de valores faltantes por columna:

colSums(is.na(data))
plot_missing(data)
```

Como puede observarse, contamos con 569 valores faltantes en la 칰ltima columna "X". M치s adelante veremos como tratarlo.

#### An치lisis exploratorio de los datos 

En este paso nuestro objetivo ser치 entender la distribuci칩n y relaciones de variables.

#### Visualizaci칩n de distribuciones y correlaciones: 

```{r}
#Veamos un resumen gr치fico general:

plot_intro(data)
```

```{r}
#Variables Categ칩ricas:

plot_bar(data)
```

```{r}
#Variables Num칠ricas
plot_histogram(data)
```

#### Preprocesamiento de los datos.

**Eliminaci칩n de valores faltantes:**

Para comenzar, ya sabemos que existe una columna cuyos valores son todos NA, es decir, faltantes. El primero paso en nuestro preprocesamiento ser치 eliminar esta columna "x":

```{r}
data <- data %>% select(-X)
```

Veamos si se ha borrado correctamente la columna X y si ahora existe alg칰n otro valor faltante:

```{r}
colnames(data)
```

```{r}
# Veamos si existen valores faltantes en nuestros datos:

anyNA(data)

#Contamos el numero de valores faltantes por columna:

plot_missing(data, title = "Valores Faltantes")
```

Adem치s de la columna con valores Faltantes, tambi칠n tenemos una columna "ID" que no nos aporta ninguna informaci칩n por lo que tambi칠n procederemos a eliminarla:

```{r}
data <- data %>% select(-id)

```

```{r}
colnames(data)
head(data)

```

Como podemos observar, se ha eliminado la columna "id" y se ha verificado que no existen mas valores faltantes exceptos los ya eliminados en "X".

Despu칠s de ellos contamos con un dataset de:

```{r}
dim(data)
```

569 filas y 31 columnas.

 Codificaci칩n de variables Categ칩ricas

Anteriormente vimos que nuestro dataset cuenta con una 칰nica columna de valores categ칩ricos. "Diagnosis" cuyos valores tienen el siguiente significado: + M (malignant) + B (benign)

El siguiente paso en el preprocesado de datos ser치 pasar esta columna a num칠rica. Como solo se presentan dos posibles valores (M y B), se aplicar치 Codificaci칩n Binaria: El valor "M" pasar치 a ser 1 y valor "B" pasar치 a ser 0.

Aunque m치s adelante volveremos a pasarla a categ칩rica, ahora es necesario hacerla binaria para poder estudiar la correlaci칩n de las variables.

```{r}

#M -> 1; B -> 0
data$diagnosis <- ifelse(data$diagnosis == "M", 1, 0)
```

Vamos a verificar que se ha realizado correctamente la conversi칩n:

```{r}
table(data$diagnosis)
# print(data$diagnosis)
```

Nos cercioramos de que no existe ning칰n otro valor categ칩rico en el dataset:

```{r}

categorical_columns <- sapply(data, is.factor) | sapply(data, is.character)
names(data)[categorical_columns]


```

Efectivamente, todas nuestras columnas ahora son num칠ricas, lo que nos da paso al siguiente punto en nuestro preprocesamiento de datos.

#### Estudio de correlaci칩n.

Al tener nuestro dataset limpio de NA y solo presentes variables num칠ricas, el siguiente paso ser치 estudiar las posibles correlaciones de nuestro dataset.

Estudiar la correlaci칩n de los datos ayuda a identificar patrones y relaciones entre variables, lo que podr칤a conducir a nuevas hip칩tesis y descubrimientos. Adem치s, un buen estudio de correlaciones podr칤a ser 칰til para seleccionar variables relevantes y construir modelos en un futuro.

Los valores que obtendremos tendr치n la siguiente interpretaci칩n:

-   Correlaci칩n cercana a 1: Relaci칩n positiva fuerte.
-   Correlaci칩n cercana a -1: Relaci칩n negativa fuerte.
-   Correlaci칩n cercana a 0: No hay relaci칩n lineal significativa

```{r}

# Calcular matriz de correlaci칩n
correlation_matrix <- cor(data, use = "complete.obs")  # Ignora valores faltantes


# Graficar matriz de correlaci칩n
corrplot(correlation_matrix, method = "color", type = "upper", tl.cex = 0.8)

```

```{r}

# Graficar la matriz de correlaci칩n con valores en las celdas
ggcorrplot(correlation_matrix, 
           method = "circle",  # Utilizar c칤rculos para representar correlaciones
           type = "lower",     # Mostrar solo la mitad inferior
           lab = TRUE,         # A침adir los valores de correlaci칩n
           lab_size = 2,       # Tama침o del texto en las celdas
           colors = c("blue", "white", "red"), # Colores para las correlaciones negativas, neutrales y positivas
           title = "Matriz de Correlaci칩n",  # T칤tulo del gr치fico
           tl.cex = 10)       # Tama침o de las etiquetas

```

Como puede observarse, al tratarese un conjunto de datos con 31 variables, la matriz de correlaci칩n cuesta interpretarla.

Por ello, como "diagnosis" es nuestra variable objetivo, vamos a observar c칩mo se correlacionan las dem치s variables con ella:

```{r}
# Calcular la correlaci칩n entre cada variable num칠rica y 'diagnosis'
cor_with_target <- cor(data, data$diagnosis, use = "complete.obs")

# Crear un data frame para ver las correlaciones junto con los nombres de las variables
correlation_df <- data.frame(Variable = names(data), Correlation = cor_with_target)

# Ordenar el data frame por la columna de Correlation en orden descendente
correlation_df_sorted <- correlation_df[order(-correlation_df$Correlation), ]

# Ver las correlaciones ordenadas junto con los nombres de las variables
print(correlation_df_sorted)


```

En esta tabla podemos ver la correlaci칩n de cada variable con "diagnosis" en orden descendente.

#### Selecci칩n de Atributos: 

Una de las formas de hacer una correcta selecci칩n de atributos es inspeccionar la anterior tabla de correlaci칩n:

Como puede observarse, no existe ninguna variable que supere el 0,8, cuando las variables se correlacionan con la variable clase, en nuestro caso diagnosis, con m치s de un 0,9, suele ser conveniente eliminarlas para evitar redundancia, lo que no es nuestro caso.

Por otro lado, tampoco tenemos valores demasiado bajos ni negativos, los valores negativos en nuestro caso, podr칤an ayudar al entrenamiento de nuestro modelo, es por ello, que de momento no se eliminar치n variables del dataset.

###  3.2. An치lisis Supervisado.

El aprendizaje supervisado, tambi칠n conocido como machine learning supervisado, es una subcategor칤a del machine learning y la inteligencia artificial. Se define por el uso de conjuntos de datos etiquetados para entrenar algoritmos que clasifican los datos o predicen los resultados con precisi칩n.

A medida que se introducen datos en el modelo, 칠ste ajusta sus ponderaciones hasta que el modelo se ha ajustado adecuadamente, lo que ocurre como parte del proceso de validaci칩n cruzada.

En nuestro an치lisis, la variable a predecir ser치 "diagnosis", que como vimos anteriormente, es de tipo binario, tomando 1 cuando el tumor es maligno, y 0 cuando el tumor resulta benigno.

En este an치lisis se han evaluado diferentes modelos de clasificaci칩n para un conjunto de datos binario con 569 muestras y 30 predictores, clasificados en dos categor칤as: 'Negative' y 'Positive'. Los modelos analizados son el 츼rbol de Decisi칩n (CART), Random Forest (Bosque Aleatorio), M치quinas de Soporte Vectorial con n칰cleo Radial (SVM), y el Modelo Lineal Generalizado (GLM).

El primer paso para el an치lisis supervisado es realizar la divi칩n del dataset.

#### 3.2.1 Divisi칩n del Dataset: 

Para este paso, usaremos la validaci칩n cruzada k-fold para dividir el dataset y evaluar el modelo. Aunque podr칤amos dividir el conjunto en un 80% train y un 20% train, valores t칤picos, se ha optado por utilizar K-fold ya que asegura que cada subconjunto o fold del dataset es utilizado tanto para el entrenamiento como para prueba en distintas iteraciones, lo que nos proporcionar치 una evaluaci칩n m치s robusta.

```{r}
# Establecemos una semilla para asegurar que los resultados sean reproducibles
set.seed(123)

k <- 5

# Configurar K-fold Cross-Validation con probabilidades de clase
train_control <- trainControl(
  method = "cv",           # Cross-validation
  number = k,              # N칰mero de pliegues (folds)
  classProbs = TRUE,       # Habilitar probabilidades de clase
  summaryFunction = twoClassSummary, # Para m칠tricas de clasificaci칩n binaria
  savePredictions = "final" # Guardar las predicciones finales
)


```

#### 3.2.2 Entrenamiento del modelo: 

Cambiamos los niveles del factor "diagnosis" a nombres v치lidos, en este caso "M" para los tumores malignos, "B" para aquellos benignos.

```{r}



# Cambiamos los niveles del factor "diagnosis" de 0 y 1 a "B" y "M"
data$diagnosis <- factor(data$diagnosis, levels = c(0, 1), labels = c("B", "M"))
```

#### 3.2.2.1 츼rbol de Decisi칩n (CART): 

El 치rbol de decisi칩n es un modelo simple que divide los datos en segmentos basados en reglas de decisi칩n. Es 칰til para clasificaciones donde las decisiones son l칩gicas y f치ciles de entender

```{r}

# 츼rbol de Decisi칩n utilizando K-fold cross-validation
model_cart <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "rpart",     # 츼rbol de decisi칩n (CART)
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_cart)

# Extraer la importancia de las variables
importance_cart <- varImp(model_cart, scale = FALSE)
importance_cart_df <- importance_cart$importance
importance_cart_df$variable <- rownames(importance_cart_df)

```

El primer modelo analizado es el 츼rbol de Decisi칩n (CART). Este modelo utiliza un valor de complejidad de poda (cp) de 0.0047, que fue seleccionado como el mejor par치metro mediante validaci칩n cruzada.

La curva ROC del modelo es de 0.9362, lo que indica una alta capacidad para discriminar entre las dos clases.

La sensibilidad (capacidad del modelo para identificar correctamente las observaciones positivas) es de 0.9383, lo que sugiere que el modelo es muy eficiente para detectar las observaciones positivas, aunque algo menos efectivo que otros modelos en cuanto a la especificidad. De hecho, la especificidad (capacidad para identificar correctamente las observaciones negativas) es de 0.8960, lo que representa una leve ca칤da respecto a la sensibilidad.

Este desempe침o es s칩lido y equilibrado, pero no es el m치s alto entre los modelos evaluados.

```         
#### 3.2.2.2 Random Forest  
```

El Random Forest es un algoritmo que construye m칰ltiples 치rboles de decisi칩n y realiza una predicci칩n agregando las predicciones de todos los 치rboles individuales. Es robusto ante el sobreajuste.

```{r}
model_rf <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "rf",        # Random Forest
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_rf)

# Extraer la importancia de las variables
importance_rf <- varImp(model_rf, scale = FALSE)
importance_rf_df <- importance_rf$importance
importance_rf_df$variable <- rownames(importance_rf_df)

```

Para entender mejor los resultados del modelo, aclarar que el par치metro mtry en el contexto de Random Forest es uno de los hiperpar치metros clave que se utiliza para controlar el n칰mero de variables (caracter칤sticas) que el modelo considera para dividir cada nodo en cada 치rbol del bosque. Espec칤ficamente, mtry define cu치ntas caracter칤sticas ser치n elegidas aleatoriamente para cada nodo cuando se construye un 치rbol en el Random Forest.

Random Forest, muestra un desempe침o destacable. Este modelo seleccion칩 el valor de mtry (n칰mero de variables aleatorias para cada divisi칩n del 치rbol) igual a 2, lo que optimiza la capacidad de discriminaci칩n. Su curva ROC alcanza un valor impresionante de 0.9908, lo que es un indicador claro de su capacidad para separar las dos clases con gran precisi칩n. Adem치s, la sensibilidad de 0.9804 muestra que Random Forest tiene una excelente capacidad para detectar correctamente las observaciones positivas, y la especificidad de 0.9241 indica que tambi칠n es eficaz en identificar las observaciones negativas. Este modelo sobresale por su alta precisi칩n en ambos aspectos, lo que lo convierte en uno de los modelos m치s robustos y confiables para este conjunto de datos.

#### 3.2.2.3 Support Vector Machine (SVM) 

El Support Vector Machine (SVM) es un algoritmo que intenta encontrar un hiperplano que mejor separe las diferentes clases de datos.

```{r}
#install.packages("kernlab")
library(kernlab)

model_svm <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "svmRadial",  # Support Vector Machine con kernel radial
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_svm)

# Extraer la importancia de las variables
importance_svm <- varImp(model_svm, scale = FALSE)
importance_svm_df <- importance_svm$importance
importance_svm_df$variable <- rownames(importance_svm_df)

```

El tercer modelo evaluado es el de M치quinas de Soporte Vectorial con n칰cleo Radial (SVM). Este modelo, con un par치metro de regularizaci칩n 洧냤=1 y un valor de sigma de 0.0475, mostr칩 un desempe침o excelente en t칠rminos de la curva ROC, alcanzando un valor de 0.9948, el m치s alto entre todos los modelos. Esta m칠trica refleja una capacidad de discriminaci칩n superior, lo que implica que el modelo tiene una alta habilidad para separar correctamente las clases 'Negative' y 'Positive'. La sensibilidad de 0.9748 y la especificidad de 0.9670 tambi칠n son notablemente altas, lo que sugiere que el modelo tiene un buen rendimiento tanto en la detecci칩n de las observaciones positivas como en la correcta identificaci칩n de las negativas. Sin embargo, es importante destacar que el modelo SVM present칩 varias advertencias durante el proceso de optimizaci칩n (warnings), relacionadas con problemas de convergencia y probabilidades extremas de 0 o 1. Esto podr칤a indicar que el modelo podr칤a estar sobreajustando o enfrentando dificultades para encontrar un equilibrio estable, lo que debe tenerse en cuenta al evaluar su estabilidad y generalizaci칩n.

#### 3.2.2.4 Regresi칩n Log칤stica 

Este algoritmo es un modelo de clasificaci칩n que predice la probabilidad de que una observaci칩n pertenzca a una clase o no.

```{r}


model_logit <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "glm",       # Regresi칩n Log칤stica
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_logit)

# Extraer la importancia de las variables
importance_logit <- varImp(model_logit, scale = FALSE)
importance_logit_df <- importance_logit$importance
importance_logit_df$variable <- rownames(importance_logit_df)

```

El 칰ltimo modelo considerado es el Modelo Lineal Generalizado (GLM). Este modelo, a pesar de ser sencillo en su estructura, mostr칩 un rendimiento respetable. Su curva ROC alcanz칩 un valor de 0.9552, lo cual es inferior a los de Random Forest y SVM, pero sigue siendo adecuado para tareas de clasificaci칩n. La sensibilidad de 0.9438 indica que el modelo tiene una buena capacidad para identificar las observaciones positivas, mientras que la especificidad de 0.9484 es ligeramente mejor que la sensibilidad, lo que sugiere que el modelo tiene un desempe침o ligeramente mejor para detectar las observaciones negativas en comparaci칩n con las positivas. Aunque el modelo GLM es funcional, su rendimiento en t칠rminos de la curva ROC es algo inferior en comparaci칩n con los modelos m치s complejos como SVM y Random Forest.

#### 3.2.2.5 Red Neuronal
Las redes neuronales son un modelo de aprendizaje profundo que imita el funcionamiento del cerebro humano. Est치n compuestas por capas de neuronas interconectadas que procesan la informaci칩n y aprenden a partir de los datos.
Puede ser interesante evaluar el rendimiento de una red neuronal en comparaci칩n con los modelos tradicionales de aprendizaje supervisado.

```{r}
# Entrenar una red neuronal para clasificaci칩n binaria
model_nn <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data,
  method = "nnet",      # Red Neuronal
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC",       # Evaluar utilizando AUC (츼rea bajo la curva ROC)
  verbose = FALSE       # Suprimir los detalles del entrenamiento
)


# Imprimir los resultados del modelo
print(model_nn$results)


# Extraer la importancia de las variables
importance_nn <- varImp(model_nn, scale = FALSE)
importance_nn_df <- importance_nn$importance
importance_nn_df$variable <- rownames(importance_nn_df)


```


#### 2.3 Comparaci칩n de Modelos 

```{r}
# Comparar el rendimiento de los modelos
resamples <- resamples(list(cart = model_cart, rf = model_rf, svm = model_svm, logit = model_logit, nn = model_nn))

# Ver el resumen de la comparaci칩n entre los modelos
summary(resamples)

# Visualizaci칩n de la comparaci칩n
bwplot(resamples)

```

#### 2.4 Estudio de la Importancia de las Variables 

El estudio de la importancia de variables nos permite identificar cu치les son las caracter칤sticas que m치s influyen en las predicciones realizadas. Esto es especialmente 칰til cuando trabjamos con datos de 치mbito sanitario, ya que nos permite priorizar las variables m치s relevantes para realizar diagn칩sticos o tomar decisiones informadas.

El objetivo del estudio de importancia de variables es eliminar aquellas que consistentemente tienen baja importancia en todos los modelos:

1- Obtener la importancia de variables de cada modelo.

#### 2.4.1 Estudio de la Importancia de las Variables con 츼rbol de Decisi칩n (CART) 

En los 치rboles de decisi칩n, la importancia de vairables se calcula en funci칩n de las ganancias de reducci칩n de impureza, por ejemplo, la reducci칩n de la entrop칤a o del 칤ndice Gini en los nodos donde la variables es utilizada para dividr los datos.

```{r}
plot(importance_cart, main = "Importancia de Variables - CART")
```

#### 2.4.2 Estudio de la Importancia de las Variables con Random Forest 

En Random Forest la importancia se calcula mediante dos enfoques comunes:

-   Importancia basada en permutaci칩n: Eval칰a c칩mo cambia la precisi칩n del modelo al permutar aleatoriamente los valores de una variable.
-   Reducci칩n promedio de la impureza: Calcula cu치nto contribuye una variable a la reducci칩n de impureza a trav칠s de todos los 치rboles del bosque.

```{r}
plot(importance_rf, main = "Importancia de Variables - Random Forest")
```

#### 2.4.3 Estudio de la Importancia de las Variables con Support Vector Machine (SVM) 

El calculo de la importancia en SVM no es tan directo, ya que este modelo no se basa en una estructura jer치rquica o en una gregaci칩n de 치rboles. Podemos estimar la importancia de las variables mediante an치lisis post-hoc, como la evaluaci칩n de los coeficientes en el espacio formado por el n칰cleo radial.

```{r}
plot(importance_svm, main = "Importancia de Variables- SVM")
```

#### 2.4.4 Estudio de la Importancia de las Variables con Regresi칩n Log칤stica 

En regresi칩n log칤stica, la importancia de variables se puede analizar mediante los coeficientes estimados del modelo. Estos coeficientes indican la magnitud y la direcci칩n del efecto de cada variable en la probabilidad de que un tumor sea maligno.

```{r}
plot(importance_logit, main = "Importancia de Variables - GLM")
```

#### 2.4.5 Estudio de la Importancia de las Variables con Red Neuronal

En las redes neuronales, la importancia de las variables puede ser m치s dif칤cil de interpretar debido a la complejidad del modelo. Sin embargo, es posible analizar la contribuci칩n de cada variable a la salida de la red mediante t칠cnicas de backpropagation y an치lisis de sensibilidad.

```{r}
plot(importance_nn, main = "Importancia de Variables - Red Neuronal")
```

#### 2.4.4 Comparaci칩n General del estudio de la Importancia de las Variables 

2- Normalizar la importancia para compararla entre modelos. 3- Calcular una m칠trica consolidada de importancia promedio. 4- Identificar las variables con menor impacto en todos los modelos.

```{r}
# Consolidar importancia de variables
importance_combined <- merge(
  merge(importance_cart_df, importance_rf_df, by = "variable", suffixes = c("_cart", "_rf")),
  merge(importance_svm_df, importance_logit_df, by = "variable", suffixes = c("_svm", "_logit")),
  
  by = "variable"
)

importance_combined <- merge(importance_combined, importance_nn_df, by = "variable")

# Promedio de importancia
importance_combined$mean_importance <- rowMeans(importance_combined[, -1], na.rm = TRUE)

# Seleccionar las menos importantes (por debajo de un umbral, por ejemplo, el percentil 20)
threshold <- quantile(importance_combined$mean_importance, 0.2)
least_important_vars <- importance_combined$variable[importance_combined$mean_importance <= threshold]

# Eliminar estas variables del dataset original
data_reduced <- data[, !(names(data) %in% least_important_vars)]

# Guardar el dataset reducido
write.csv(data_reduced, "data_reduced.csv", row.names = FALSE)

```

```{r}
data_reduced
dim(data_reduced)

```

```{r}
colnames(data_reduced)
```

```{r}
colnames(data)
```

El siguiente paso ser치 repetir nuestro an치lisis supervisado esta vez usando el dataset reducido en el que no aparecen las columnas "menos importantes". De esta forma podremos analizar si los resultados mejorar, empeoran o no afectan con el estudio de importancia de varianles:

#### 3. An치lisis Supervisado con dataset reducido.  
#### 3.1 츼rbol de Decisi칩n (CART) 

```{r}
# 츼rbol de Decisi칩n utilizando K-fold cross-validation
model_cart_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "rpart",     # 츼rbol de decisi칩n (CART)
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_cart_reduced)
```

#### 3.2 Random Forest 

```{r}
model_rf_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "rf",        # Random Forest
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_rf_reduced)

```

#### 3.3 Support Vector Machine (SVM) 

```{r}
#install.packages("kernlab")
library(kernlab)

model_svm_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "svmRadial",  # Support Vector Machine con kernel radial
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_svm_reduced)

```

#### 3.4 Regresi칩n Log칤stica 

```{r}
model_logit_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "glm",       # Regresi칩n Log칤stica
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC"        # Evaluar utilizando AUC (츼rea bajo la curva ROC)
)

# Ver el resumen del modelo entrenado
print(model_logit_reduced)
```
#### 3.5 Red Neuronal

```{r}
# Entrenar una red neuronal para clasificaci칩n binaria
model_nn_reduced <- train(
  diagnosis ~ .,        # Usamos todas las variables predictoras
  data = data_reduced,
  method = "nnet",      # Red Neuronal
  trControl = train_control,  # Control de validaci칩n cruzada
  metric = "ROC",       # Evaluar utilizando AUC (츼rea bajo la curva ROC)
  verbose = FALSE       # Suprimir los detalles del entrenamiento
)

```

#### 3.5 Comparaci칩n de resultados an치lisis supervisado dataset incial vs dataset reducido: 

```{r}
# Comparativa entre los modelos
# Lista de los modelos completos y reducidos
modelos_completos <- list(cart = model_cart, rf = model_rf, svm = model_svm, logit = model_logit, nn = model_nn)
modelos_reducidos <- list(cart = model_cart_reduced, rf = model_rf_reduced, svm = model_svm_reduced, logit = model_logit_reduced, nn = model_nn_reduced)

# Inicializamos un data frame vac칤o para almacenar los resultados
resultados <- data.frame(
  modelo = character(),
  roc_inicial = numeric(),
  sens_inicial = numeric(),
  spec_inicial = numeric(),
  roc_reducido = numeric(),
  sens_reducido = numeric(),
  spec_reducido = numeric(),
  stringsAsFactors = FALSE
)

# Iteramos sobre los modelos completos y reducidos
for (nombre in names(modelos_completos)) {
  # Modelo completo
  modelo_completo <- modelos_completos[[nombre]]
  mejor_completo <- modelo_completo$results[which.max(modelo_completo$results$ROC), c("ROC", "Sens", "Spec")]
  
  # Modelo reducido
  modelo_reducido <- modelos_reducidos[[nombre]]
  mejor_reducido <- modelo_reducido$results[which.max(modelo_reducido$results$ROC), c("ROC", "Sens", "Spec")]
  
  # Agregar al data frame
  resultados <- rbind(resultados, data.frame(
    modelo = nombre,
    roc_inicial = mejor_completo$ROC,
    sens_inicial = mejor_completo$Sens,
    spec_inicial = mejor_completo$Spec,
    roc_reducido = mejor_reducido$ROC,
    sens_reducido = mejor_reducido$Sens,
    spec_reducido = mejor_reducido$Spec,
    stringsAsFactors = FALSE
  ))
}

# Imprimir los resultados
print(resultados)

# Una tabla mostrando el relative change de cada m칠trica
comparativa <- resultados %>%
  mutate(
    roc_change = (roc_reducido - roc_inicial) / roc_inicial ,
    sens_change = (sens_reducido - sens_inicial) / sens_inicial,
    spec_change = (spec_reducido - spec_inicial) / spec_inicial
  ) %>%
  select(modelo, roc_change, sens_change, spec_change)

print(comparativa)

```

```{r}
# Instalar flextable si no est치 instalado
install.packages("flextable")

# Cargar la librer칤a
library(flextable)

# Crear la tabla con flextable
tabla_flex <- flextable(comparativa)

# Imprimir la tabla en un entorno que soporte flextable (por ejemplo, RStudio)
tabla_flex

```

-   CART:

La sensibilidad mejora con el dataset reducido, lo que indica una mejor capacidad para detectar casos positivos. Esto puede deberse a que las variables menos relevantes generaban divisiones innecesarias en el 치rbol.

-   Random Forest:

Aunque hay una leve disminuci칩n en ROC y sensibilidad, la especificidad mejora. Esto implica que el modelo con el dataset reducido identifica mejor los casos negativos.

-   SVM:

El impacto del dataset reducido es m칤nimo, pero muestra una ligera disminuci칩n en todas las m칠tricas. Esto sugiere que las variables eliminadas ten칤an una influencia menor, pero no eran completamente irrelevantes.

-   GLM:

El modelo con dataset reducido muestra mejoras claras en todas las m칠tricas, lo que refuerza la idea de que las variables eliminadas no eran 칰tiles para este enfoque lineal.

En resumen: La eliminaci칩n de variables menos importantes mantiene un rendimiento comparable, e incluso mejora algunas m칠tricas espec칤ficas en ciertos modelos.

Esto sugiere que las variables eliminadas no aportaban informaci칩n relevante o incluso pod칤an introducir ruido.

Reducir el n칰mero de variables tiene ventajas pr치cticas, como menores requerimientos computacionales y mayor interpretabilidad, sin comprometer el rendimiento.

###  4. An치lisis No Supervisado.

El an치lisis no supervisado tiene como objetivo descubrir patrones ocultos o estructuras en los datos sin la necesidad de etiquetas. En este caso, aplicaremos t칠cnicas de clustering y reducci칩n de dimensionalidad para explorar la informaci칩n contenida en las caracter칤sticas de las c칠lulas. Estas t칠cnicas nos ayudar치n a identificar grupos de observaciones similares y a obtener una visi칩n m치s clara de las relaciones entre las variables.

#### 4.1 Preparaci칩n del Dataset 

Antes de aplicar cualquier t칠cnica de an치lisis no supervisado, es necesario realizar un preprocesamiento adecuado de los datos. El primer paso es eliminar la columna "diagnosis", ya que esta variable contiene la etiqueta que queremos predecir y no se utiliza en el an치lisis no supervisado. A continuaci칩n, normalizamos las caracter칤sticas, ya que muchas t칠cnicas, como el clustering, son sensibles a las escalas de las variables. Esto asegura que todas las variables tengan la misma influencia en el modelo.

```{r}
data_UnSupervised <- data %>% select(-diagnosis)
head(data_UnSupervised)
data_UnSupervised$diagnosis
```
La normalizaci칩n permite que las variables con diferentes unidades de medida no dominen el an치lisis y garantiza que el algoritmo de clustering o reducci칩n de dimensionalidad no se vea sesgado por la magnitud de las variables.
```{r}
# Normalizar los datos (sin la variable de respuesta si existe)
preprocess_params <- preProcess(data_UnSupervised[, -ncol(data_UnSupervised)], method = c("center", "scale"))
data_normalized <- predict(preprocess_params, data_UnSupervised[, -ncol(data_UnSupervised)])

```

#### 4.2 Clustering: Determinaci칩n del N칰mero 칍ptimo de Clusters 

El primer paso en cualquier t칠cnica de clustering es determinar el n칰mero adecuado de grupos (clusters). Utilizamos dos m칠todos comunes para esto:

M칠todo del codo: Este m칠todo muestra la variaci칩n explicada en funci칩n del n칰mero de clusters. En general, se busca el "codo" de la gr치fica, el punto en el que la mejora en la varianza explicada se estabiliza, indicando que a침adir m치s clusters no mejora significativamente la segmentaci칩n.

칈ndice de Silhouette: Este 칤ndice mide c칩mo de similar es cada punto a su propio cluster en comparaci칩n con los puntos de otros clusters. Un valor cercano a +1 indica que los puntos est치n bien agrupados, mientras que un valor cercano a -1 indica que los puntos podr칤an estar mal agrupados.

```{r}
# M칠todo del codo
wss <- (nrow(data_normalized) - 1) * sum(apply(data_normalized, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(data_normalized, centers = i)$withinss)

plot(1:15, wss, type = "b", xlab = "N칰mero de Clusters", ylab = "Suma de Cuadrados Internos")

# 칈ndice silhouette
library(cluster)
silhouette_scores <- numeric()
for (i in 2:15) {
  km <- kmeans(data_normalized, centers = i)
  silhouette_scores[i] <- mean(silhouette(km$cluster, dist(data_normalized))[, 3])
}
plot(2:15, silhouette_scores[-1], type = "b", xlab = "N칰mero de Clusters", ylab = "Puntaje de Silhouette")

```

#### 4.3 K-Means 

Una vez determinado el n칰mero 칩ptimo de clusters, implementamos el algoritmo de K-means. Este es un algoritmo de clustering que agrupa los datos en K clusters, minimizando la varianza dentro de cada grupo. Es importante mencionar que, aunque este es un algoritmo ampliamente utilizado, la interpretaci칩n de los resultados debe hacerse con cautela, ya que el n칰mero de clusters es un valor sensible a la inicializaci칩n y la distribuci칩n de los datos.
```{r}
set.seed(123)

optimal_clusters = 4

kmeans_model <- kmeans(data_normalized, centers = optimal_clusters, nstart = 25)

# Agregar etiquetas de cluster al dataset original
data$cluster <- kmeans_model$cluster

# Mostrar los clusters de diferentes formas:

# Tabla de frecuencias
table(data$cluster)

# Resumen de los datos agrupados por cluster
aggregate(data[, -ncol(data)], by = list(cluster = data$cluster), FUN = mean)


```




#### 4.4 PCA 

El pr칩ximo paso de nuestro an치lisis no supervisado ser치 utilizar el An치lisis de Componentes Principales para visualizar la estructura de los datos y validar los resultados del clustering.

```{r}
# PCA
pca_model <- prcomp(data_normalized, scale = TRUE)

# Visualizaci칩n de los dos primeros componentes principales
pca_data <- data.frame(pca_model$x[, 1:2], cluster = as.factor(data$cluster))
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Clustering visualizado en espacio PCA")
table(data$diagnosis, data$cluster)


```

Se visualizan claramente cuatro cl칰steres.

Los cl칰steres est치n bien definidos y separados espacialmente, especialmente entre el cl칰ster p칰rpura y los dem치s.

Hay un grado menor de solapamiento entre los cl칰steres, lo que sugiere que las variables originales ofrecen una separaci칩n m치s clara entre las clases en este espacio. Respecto al espacio de Variaci칩n, los puntos est치n distribuidos en torno a coordenadas m치s centradas (de -15 a 5 en PC1 y -10 a 10 en PC2).

Vamos tambi칠n a repetir el clustering con el dataset reducido generado durante el estudio de la importancia de las variables:

#### 4.5 An치lisis No Supervisado con Dataset Reducido seg칰n importancia de variables 

```{r}
data_UnSupervised_reduced <- data_reduced %>% select(-diagnosis)
head(data_UnSupervised_reduced)
data_UnSupervised_reduced$diagnosis
```

```{r}
# Normalizar los datos (sin la variable de respuesta si existe)
preprocess_params <- preProcess(data_UnSupervised_reduced[, -ncol(data_UnSupervised_reduced)], method = c("center", "scale"))
data_normalized_reduced <- predict(preprocess_params, data_UnSupervised_reduced[, -ncol(data_UnSupervised_reduced)])
```

```{r}
# M칠todo del codo
wss <- (nrow(data_normalized_reduced) - 1) * sum(apply(data_normalized_reduced, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(data_normalized_reduced, centers = i)$withinss)

plot(1:15, wss, type = "b", xlab = "N칰mero de Clusters", ylab = "Suma de Cuadrados Internos")

# 칈ndice silhouette
library(cluster)
silhouette_scores <- numeric()
for (i in 2:15) {
  km <- kmeans(data_normalized_reduced, centers = i)
  silhouette_scores[i] <- mean(silhouette(km$cluster, dist(data_normalized_reduced))[, 3])
}
plot(2:15, silhouette_scores[-1], type = "b", xlab = "N칰mero de Clusters", ylab = "Puntaje de Silhouette")
```

k-means

```{r}
set.seed(123)

optimal_clusters_2 = 5

kmeans_model_reduced <- kmeans(data_normalized_reduced, centers = optimal_clusters_2, nstart = 25)

# Agregar etiquetas de cluster al dataset original
data_reduced$cluster <- kmeans_model_reduced$cluster
```

```{r}
# PCA
pca_model_reduced <- prcomp(data_normalized_reduced, scale = TRUE)

# Visualizaci칩n de los dos primeros componentes principales
library(ggplot2)
pca_data_reduced <- data.frame(pca_model_reduced$x[, 1:2], cluster = as.factor(data_reduced$cluster))
ggplot(pca_data_reduced, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point() +
  theme_minimal() +
  labs(title = "Clustering visualizado en espacio PCA")
```

En este caso, se visualizan cinco cl칰steres, lo que podr칤a indicar una mayor sensibilidad del modelo al agrupar los datos reducidos.

Los cl칰steres est치n m치s compactos en el centro del gr치fico, con menos separaci칩n clara en comparaci칩n con la versi칩n completa.

Existen cl칰steres adicionales (como el cl칰ster 5, representado en rosa), lo que puede indicar una partici칩n m치s forzada o sensibilidad al ruido.

Hay un mayor solapamiento entre los cl칰steres en comparaci칩n con el gr치fico sin reducci칩n de datos.

El rango del eje PC1 var칤a de -5 a 15, mientras que PC2 tiene una distribuci칩n de -5 a 15, lo que sugiere un cambio en la importancia relativa de los componentes principales despu칠s de la reducci칩n de variables.

En conclusi칩n, sin reducci칩n, los cl칰steres son m치s definidos y separados, lo que sugiere que las variables eliminadas contienen informaci칩n relevante para distinguir los grupos.

Con reducci칩n, los cl칰steres muestran mayor solapamiento y parecen menos definidos, posiblemente debido a la p칠rdida de informaci칩n al reducir las variables.

La aparici칩n de un quinto cl칰ster en el conjunto reducido indica que la reducci칩n puede introducir sensibilidad al ruido o revelar patrones m치s complejos (aunque menos significativos).

###  5.Reglas de asociaci칩n con algoritmo Apriori.

Las reglas de asociaci칩n son usadas en Data Mining para identificar relaciones entre elementos de un conjunto de datos. Estas relaciones, a su vez, nos ayudan a encontrar patrones de datos.

Estan formadas por un antecedente {a} y un consecuente {b}, de manera que {a} implica -\> {b} Tambi칠n es conveniente conocer los t칠rminos de: - Soporte: Proporci칩n de transacciones que contienen ambos conjuntos de 칤tems (antecedente y consecuente) - Confianza: Aciertos o c칩mo de bien predice el antecedente al consecuente.

### 5.1 Algoritmo Apriori 

Haremos uso del Algoritmo Apriori para obtener las reglas de asociaci칩n ya que es 칰til para encontrar itemsets frecuentes en datos transaccionales (datos tipo eventos en un intervalo de tiempo determinado)

#### 5.1.1 Fase 1: Reducci칩n del n칰mero de candidatos 

Generar todos los itemsets con 1 칰nico 칤tem. Posteriormente, se combinar치n estos itemstes formando itemsets con 2 elementos y as칤 sucesivamente. Nos quedaremos con los pares cuyo soporte sea mayor o igual a un minsup (eliminando aquellos que no cumplen con el umbral de soporte)

1.  Intalaremos el paquete necesario (arules) paquete en R que proporciona la funcionalidad para trabajar con reglas de asociaci칩n.

```{r}
#install.packages("arules")
library(arules)
```

2.  Cargamos los datos como transacciones: Formato basket: Cada transacci칩n se interpreta como un conjunto de 칤tems (basket), permitiendo tanto datos categ칩ricos (diagnosis) como continuos (el resto) en la misma transacci칩n.

```{r}
transacciones <- read.transactions("data_reduced.csv", format = "basket", sep = ",")

```

#### 5.1.2 Fase 2: Generar reglas  Par치metros: soporte m칤nimo (0.01 -\> 1%) y confianza m칤nima (80%)

```{r}
reglasAsociacion <- apriori(transacciones, parameter = list(supp = 0.01, conf = 0.8))

```

Filtramos las reglas basadas en el lift y la confianza, as칤 solo se consideran aquellas con un lift mayor que 1.5 y una confianza superior al 80%. lift \> 1.5 fuerte asociaci칩n entre los 칤tems en la regla. confidence \>= 0.8 asegura que la probabilidad de que el consecuente ocurra cuando el antecedente est치 presente es alta.

```{r}
filtradas <- subset(reglasAsociacion, lift > 1.5 & confidence >= 0.8)
inspect(filtradas)
```
